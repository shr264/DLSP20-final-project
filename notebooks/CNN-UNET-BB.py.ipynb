{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import psutil\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numbers\n",
    "from torchvision.transforms import functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "#image_folder = '../../DLSP20Dataset/data'\n",
    "#annotation_csv = '../../DLSP20Dataset/data/annotation.csv'\n",
    "\n",
    "#azure\n",
    "image_folder = '../../data'\n",
    "annotation_csv = '../../data/annotation.csv'\n",
    "\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "\n",
    "def order_points(pts):\n",
    "    from scipy.spatial import distance as dist\n",
    "    import numpy as np\n",
    "    \n",
    "    xSorted = pts[np.argsort(pts[:, 0]), :]\n",
    "\n",
    "    leftMost = xSorted[:2, :]\n",
    "    rightMost = xSorted[2:, :]\n",
    "\n",
    "    leftMost = leftMost[np.argsort(leftMost[:, 1]), :]\n",
    "    (tl, bl) = leftMost\n",
    "\n",
    "    D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0]\n",
    "    (br, tr) = rightMost[np.argsort(D)[::-1], :]\n",
    "\n",
    "    return np.array([tl, tr, br, bl], dtype=\"float32\")\n",
    "\n",
    "def arrange_box(x1,y1):\n",
    "    box=np.array(list(zip(x1,y1)))\n",
    "    box=order_points(box)\n",
    "    return box\n",
    "\n",
    "def iou(box1, box2):\n",
    "    from shapely.geometry import Polygon\n",
    "    a = Polygon(torch.t(box1)).convex_hull\n",
    "    b = Polygon(torch.t(box2)).convex_hull\n",
    "    \n",
    "    return a.intersection(b).area / a.union(b).area\n",
    "\n",
    "#def iou(xy1,xy2):\n",
    "#    \n",
    "#    from shapely.geometry import Polygon\n",
    "#    \n",
    "#    boxA = Polygon(arrange_box(xy1[0],xy1[1])).buffer(1e-9)\n",
    "#    boxB = Polygon(arrange_box(xy2[0],xy2[1])).buffer(1e-9)\n",
    "#    \n",
    "#    try:\n",
    "#        return boxA.intersection(boxB).area / boxA.union(boxB).area\n",
    "#    except:\n",
    "#        print('Box 1:',xy1[0],xy1[1])\n",
    "#        print('Box 2:',xy2[0],xy2[1])\n",
    "#        sys.exit(1)\n",
    "\n",
    "\n",
    "class NRandomCrop(object):\n",
    "\n",
    "    def __init__(self, size, n=1, padding=0, pad_if_needed=False):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "        self.padding = padding\n",
    "        self.pad_if_needed = pad_if_needed\n",
    "        self.n = n\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(img, output_size, n):\n",
    "        w, h = img.size\n",
    "        th, tw = output_size\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i_list = [random.randint(0, h - th) for i in range(n)]\n",
    "        j_list = [random.randint(0, w - tw) for i in range(n)]\n",
    "        return i_list, j_list, th, tw\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.padding > 0:\n",
    "            img = F.pad(img, self.padding)\n",
    "\n",
    "        # pad the width if needed\n",
    "        if self.pad_if_needed and img.size[0] < self.size[1]:\n",
    "            img = F.pad(img, (int((1 + self.size[1] - img.size[0]) / 2), 0))\n",
    "        # pad the height if needed\n",
    "        if self.pad_if_needed and img.size[1] < self.size[0]:\n",
    "            img = F.pad(img, (0, int((1 + self.size[0] - img.size[1]) / 2)))\n",
    "\n",
    "        i, j, h, w = self.get_params(img, self.size, self.n)\n",
    "\n",
    "        return n_random_crops(img, i, j, h, w)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(size={0}, padding={1})'.format(self.size, self.padding)\n",
    "\n",
    "def map_to_ground_truth(overlaps, print_it=False):\n",
    "    prior_overlap, prior_idx = overlaps.max(1)\n",
    "    if print_it: print(prior_overlap)\n",
    "#     pdb.set_trace()\n",
    "    gt_overlap, gt_idx = overlaps.max(0)\n",
    "    gt_overlap[prior_idx] = 1.99\n",
    "    for i,o in enumerate(prior_idx): gt_idx[o] = i\n",
    "    return gt_overlap,gt_idx\n",
    "\n",
    "def calculate_overlap(target_bb, predicted_bb):\n",
    "    overlaps = torch.zeros(target_bb.size(0),predicted_bb.size(0))\n",
    "\n",
    "    for j in range(overlaps.shape[0]):\n",
    "        for k in range(overlaps.shape[1]):\n",
    "            overlaps[j][k] = iou(target_bb[j],predicted_bb[k])\n",
    "            \n",
    "    return overlaps\n",
    "\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels.data.cpu()]\n",
    "\n",
    "from skimage import draw\n",
    "import numpy as np\n",
    "\n",
    "def poly2mask(vertex_row_coords, vertex_col_coords, shape):\n",
    "    fill_row_coords, fill_col_coords = draw.polygon(vertex_row_coords, vertex_col_coords, shape)\n",
    "    mask = torch.zeros(shape, dtype=np.bool)\n",
    "    mask[fill_row_coords, fill_col_coords] = True\n",
    "    return mask\n",
    "\n",
    "def convert_to_binary_mask(corners, shape=(800,800)):\n",
    "    point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2], corners[:, 0]])\n",
    "    x,y = point_squence.T[0].detach() * 10 + 400, -point_squence.T[1].detach() * 10 + 400\n",
    "    new_im = poly2mask(y, x, shape)\n",
    "    return new_im\n",
    "\n",
    "def create_conf_matrix(target, pred, debug=True):\n",
    "    import sys\n",
    "    \n",
    "    target = target.reshape(-1)\n",
    "    pred = pred.reshape(-1)\n",
    "    \n",
    "    if debug:\n",
    "        print('Target values:', target.unique())\n",
    "        print('Predicted values:', pred.unique())\n",
    "        print('Target shape:', target.shape)\n",
    "        print('Predicted shape:', pred.shape)\n",
    "    \n",
    "    nb_classes = max(target.unique())\n",
    "    if len(pred.unique()) > (nb_classes+1) :\n",
    "        print('More predicted classes than true classes')\n",
    "        sys.exit(1)\n",
    "        \n",
    "    conf_matrix = torch.zeros(nb_classes+1, nb_classes+1)\n",
    "    for t, p in zip(target, pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "def create_conf_matrix2(target, pred, debug=True):\n",
    "    import sys\n",
    "    \n",
    "    target = target.reshape(-1).cpu().numpy()\n",
    "    pred = pred.reshape(-1).cpu().numpy()\n",
    "    \n",
    "        \n",
    "    conf_matrix = torch.from_numpy(confusion_matrix(target, pred)).to(device)\n",
    "    threat_score = (1.0*conf_matrix[1,1])/(conf_matrix[1,1]+conf_matrix[1,0]+conf_matrix[0,1])\n",
    "    \n",
    "    print('Threat Score: {}'.format(threat_score))\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "def classScores(conf_matrix):\n",
    "    print('Confusion matrix\\n', conf_matrix)\n",
    "    TP = conf_matrix.diag()\n",
    "    TN = torch.zeros_like(TP)\n",
    "    FP = torch.zeros_like(TP)\n",
    "    FN = torch.zeros_like(TP)\n",
    "    for c in range(conf_matrix.size(0)):\n",
    "        idx = torch.ones(conf_matrix.size(0)).byte()\n",
    "        idx[c] = 0\n",
    "        # all non-class samples classified as non-class\n",
    "        TN[c] = conf_matrix[idx.nonzero()[:, None], idx.nonzero()].sum() #conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n",
    "        # all non-class samples classified as class\n",
    "        FP[c] = conf_matrix[idx, c].sum()\n",
    "        # all class samples not classified as class\n",
    "        FN[c] = conf_matrix[c, idx].sum()\n",
    "\n",
    "        print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(\n",
    "            c, TP[c], TN[c], FP[c], FN[c]))\n",
    "        \n",
    "    return (TP.detach().cpu().numpy(), \n",
    "            TN.detach().cpu().numpy(), \n",
    "            FP.detach().cpu().numpy(), \n",
    "            FN.detach().cpu().numpy())\n",
    "\n",
    "def split_list(a_list):\n",
    "    half = len(a_list)//2\n",
    "    return a_list[:half], a_list[half:]\n",
    "\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "\n",
    "train_scene_index = np.random.choice(labeled_scene_index, int(np.ceil(0.8*len(labeled_scene_index))))\n",
    "\n",
    "test_scene_index = labeled_scene_index[np.isin(labeled_scene_index, train_scene_index,invert=True)]\n",
    "\n",
    "\n",
    "#transform=torchvision.transforms.Compose([torchvision.transforms.Resize((200,200)),\n",
    "#                                          torchvision.transforms.ToTensor(),\n",
    "#                              torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#                             ])\n",
    "\n",
    "mean, sd = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "transform = transforms.Compose(\n",
    "    [NRandomCrop(size=32, n=5, padding=4),\n",
    "     transforms.Lambda(\n",
    "         lambda crops: \n",
    "         torch.stack([transforms.Normalize(mean, sd)(torchvision.transforms.Resize((200,200)))(transforms.ToTensor()(crop)) for crop in crops])),\n",
    "     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0, \n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "labeled_testset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(labeled_testset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0, \n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/syedhr264/dl-project-private/code/data_helper.py\", line 113, in __getitem__\n    images.append(self.transform(image))\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"<ipython-input-3-1c2e9a4d2ad5>\", line 128, in __call__\n    img = F.pad(img, self.padding)\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/nn/functional.py\", line 3388, in _pad\n    assert len(pad) % 2 == 0, 'Padding length must be divisible by 2'\nTypeError: object of type 'int' has no len()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3d7cd88edb09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroad_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroad_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/syedhr264/dl-project-private/code/data_helper.py\", line 113, in __getitem__\n    images.append(self.transform(image))\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torchvision/transforms/transforms.py\", line 61, in __call__\n    img = t(img)\n  File \"<ipython-input-3-1c2e9a4d2ad5>\", line 128, in __call__\n    img = F.pad(img, self.padding)\n  File \"/home/syedhr264/anaconda3/envs/pDL/lib/python3.8/site-packages/torch/nn/functional.py\", line 3388, in _pad\n    assert len(pad) % 2 == 0, 'Padding length must be divisible by 2'\nTypeError: object of type 'int' has no len()\n"
     ]
    }
   ],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )   \n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(3, 16)\n",
    "        self.dconv_down2 = double_conv(16, 32)\n",
    "        self.dconv_down3 = double_conv(32, 48)\n",
    "        self.dconv_down4 = double_conv(48, 64)        \n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up3 = double_conv(48 + 64, 48)\n",
    "        self.dconv_up2 = double_conv(32 + 48, 32)\n",
    "        self.dconv_up1 = double_conv(32 + 16, 16)\n",
    "        \n",
    "        \n",
    "        self.dconv_up0 = double_conv(6*16, 3*16)\n",
    "        self.dconv_up00 = double_conv(3*16,16)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(16, n_class, 1)\n",
    "        \n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)  \n",
    "\n",
    "        \n",
    "        x = self.dconv_down4(x)\n",
    "\n",
    "        \n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "\n",
    "        \n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)   \n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        \n",
    "        x = self.dconv_up1(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = [self.forward_once(y) for y in x]\n",
    "        x = torch.cat(x,axis=1)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = self.dconv_up0(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = self.dconv_up00(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return torch.sigmoid(out)\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "model = UNet(1).to(device)\n",
    "# Setting the optimiser\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.5, 0.999)\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                       mode='min', \n",
    "                                                       factor=0.1, \n",
    "                                                       patience=5,\n",
    "                                                       verbose=True)\n",
    "\n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    \n",
    "    loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "\n",
    "def loss_function(x_hat, x, epoch=None):\n",
    "    #only weighted bCE for first 8 epochs\n",
    "    if epoch!=None and epoch < 15:\n",
    "        #weighted\n",
    "        weight = torch.tensor([1, 1000])\n",
    "        weight_ = weight[x.data.view(-1).long()].view_as(x).to(device)\n",
    "        BCE = nn.functional.binary_cross_entropy(\n",
    "            x_hat, x, reduction='none'\n",
    "        )\n",
    "        BCE = (BCE*weight_).mean()\n",
    "    else:\n",
    "        BCE = nn.functional.binary_cross_entropy(\n",
    "            x_hat, x, reduction='mean'\n",
    "        )\n",
    "    \n",
    "    DICE = dice_loss(x_hat, x)\n",
    "\n",
    "    return BCE + DICE\n",
    "\n",
    "\n",
    "# Training and testing the VAE\n",
    "\n",
    "road_accuracy_list = []\n",
    "bb_accuracy_list = []\n",
    "best_loss = 100000000\n",
    "threshold = 0.5\n",
    "epochs = 25\n",
    "for epoch in range(0, epochs + 1):\n",
    "    # Training\n",
    "    if epoch >= 0:  # test untrained net first\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            sample, target, road_image, extra = data\n",
    "            batch_size = len(road_image)\n",
    "            x = torch.zeros((batch_size,1,800,800))\n",
    "            #x[:,0,:,:] = 1.0*torch.stack(road_image).reshape(-1, 800, 800)\n",
    "            x[:,0,:,:] = torch.zeros((batch_size,800,800))\n",
    "            for i in range(batch_size):\n",
    "                for cat, bb in zip(target[i]['category'], target[i]['bounding_box']):\n",
    "                    x[i,0,:,:] = 1.0*convert_to_binary_mask(bb)\n",
    "            x = x.to(device)\n",
    "            y = torch.stack(sample).reshape(6,-1,3,200,200).to(device)\n",
    "            # ===================forward=====================\n",
    "            x_hat = model(y)\n",
    "            loss = loss_function(x_hat, x)\n",
    "            train_loss += loss.item()\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ===================log========================\n",
    "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(trainloader.dataset):.4f}')\n",
    "\n",
    "    means, logvars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = [0,0]\n",
    "        total = [0,0]\n",
    "        conf_matrices = [torch.zeros(2,2).to(device),torch.zeros(2,2).to(device)]\n",
    "        for batch_idx, data in enumerate(testloader):\n",
    "            sample, target, road_image, extra = data\n",
    "            batch_size = len(road_image)\n",
    "            x = torch.zeros((batch_size,1,800,800))\n",
    "            #x[:,0,:,:] = 1.0*torch.stack(road_image).reshape(-1, 800, 800)\n",
    "            x[:,0,:,:] = torch.zeros((batch_size,800,800))\n",
    "            for i in range(batch_size):\n",
    "                for cat, bb in zip(target[i]['category'], target[i]['bounding_box']):\n",
    "                    x[i,0,:,:] = 1.0*convert_to_binary_mask(bb)\n",
    "            x = x.to(device) \n",
    "            y = torch.stack(sample).reshape(6,-1,3,200,200).to(device)\n",
    "            \n",
    "            # ===================forward=====================\n",
    "            x_hat = model(y)\n",
    "            test_loss += loss_function(x_hat, x).item()\n",
    "            # =====================log=======================\n",
    "            \n",
    "            for i in x.size(1):\n",
    "                print('='*100)\n",
    "                print('Channel:{}'.format(i))\n",
    "                correct[i] += (x_hat[:,i,:,:]>threshold).eq(\n",
    "                    (x[:,i,:,:]==1).data.view_as((\n",
    "                        x_hat[:,i,:,:]>threshold))).cpu().sum().item()\n",
    "                total[i] += x[:,i,:,:].nelement()\n",
    "\n",
    "                if batch_idx % 100 == 0:\n",
    "                    for k in range(0,60):\n",
    "                        thld = 0.2+k*0.01\n",
    "                        print('Confusion Matrix at threshold: {} for Channel {}'.format(thld, i))\n",
    "                        print(create_conf_matrix2(1*(x[:,i,:,:]==1), 1*(x_hat[:,i,:,:]>thld)))\n",
    "                        print('='*50)\n",
    "                    print('='*75)\n",
    "                conf_matrices[i] += create_conf_matrix2(1*(x[:,i,:,:]==1), 1*(x_hat[:,i,:,:]>threshold))\n",
    "                print('='*100)\n",
    "            \n",
    "                       \n",
    "    bb_accuracy = 100. * correct[0] / total[0]\n",
    "    \n",
    "    if test_loss < best_loss:\n",
    "        print('Updating best model')\n",
    "        best_loss = copy.deepcopy(test_loss)\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), \n",
    "                   'models/unet_single_model.pth')\n",
    "\n",
    "        \n",
    "    scheduler.step(test_loss)\n",
    "    road_accuracy_list.append(road_accuracy)\n",
    "    bb_accuracy_list.append(bb_accuracy)\n",
    "    print(\"\"\"\\nTest set: Average loss: {:.4f}, \n",
    "    Accuracy BB: {}/{} ({:.0f}%) ,\n",
    "    BB: \n",
    "    TP {} \n",
    "    TN {}\n",
    "    FP {}\n",
    "    FN {}\"\"\".format(\n",
    "        test_loss, \n",
    "        correct[0], total[0], bb_accuracy,\n",
    "        *classScores(conf_matrices[0]) ))\n",
    "\n",
    "            #labels.append(y.detach())\n",
    "    # ===================log========================\n",
    "    #codes['y'].append(torch.cat(labels))\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print(f'====> Test set loss: {test_loss:.4f}')\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow((x[0,0,:,:].squeeze()==1).detach().cpu().numpy(), cmap='binary')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow((x_hat[0,0,:,:].squeeze()>threshold).detach().cpu().numpy(), cmap='binary')\n",
    "    plt.savefig(\"imgs/unet_single_plot_epoch_\"+str(epoch)+\".png\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    \n",
    "pd.DataFrame([bb_accuracy_list], \n",
    "             columns=['bb_accuracy']).to_csv('unet_single_accuracy_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "pdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
