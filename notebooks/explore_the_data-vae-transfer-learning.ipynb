{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final competition of Deep Learning 2020 Spring\n",
    "Traffic environment semi-supervised Learning Contest\n",
    "\n",
    "## Goals\n",
    "The objective is to train a model using images captured by six different cameras attached to the same car to generate a top down view of the surrounding area. The performance of the model will be evaluated by (1) the ability of detecting objects (like car, trucks, bicycles, etc.) and (2) the ability to draw the road map layout.\n",
    "\n",
    "## Data\n",
    "You will be given two sets of data:\n",
    "\n",
    " 1. Unlabeled set: just images\n",
    " 2. Labeled set: images and the labels(bounding box and road map layout)\n",
    "\n",
    "This notebook will help you understand the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  3 16:49:30 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 1080    On   | 00000000:88:00.0 Off |                  N/A |\r\n",
      "| 40%   60C    P2    50W / 180W |   2283MiB /  8119MiB |     42%   E. Process |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     29879      C   /home/shr264/anaconda3/envs/pDL/bin/python  2273MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import psutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '../../DLSP20Dataset/data'\n",
    "annotation_csv = '../../DLSP20Dataset/data/annotation.csv'\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "\n",
    "def order_points(pts):\n",
    "    from scipy.spatial import distance as dist\n",
    "    import numpy as np\n",
    "    \n",
    "    xSorted = pts[np.argsort(pts[:, 0]), :]\n",
    "\n",
    "    leftMost = xSorted[:2, :]\n",
    "    rightMost = xSorted[2:, :]\n",
    "\n",
    "    leftMost = leftMost[np.argsort(leftMost[:, 1]), :]\n",
    "    (tl, bl) = leftMost\n",
    "\n",
    "    D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0]\n",
    "    (br, tr) = rightMost[np.argsort(D)[::-1], :]\n",
    "\n",
    "    return np.array([tl, tr, br, bl], dtype=\"float32\")\n",
    "\n",
    "def arrange_box(x1,y1):\n",
    "    box=np.array(list(zip(x1,y1)))\n",
    "    box=order_points(box)\n",
    "    return box\n",
    "\n",
    "def iou(box1, box2):\n",
    "    from shapely.geometry import Polygon\n",
    "    a = Polygon(torch.t(box1)).convex_hull\n",
    "    b = Polygon(torch.t(box2)).convex_hull\n",
    "    \n",
    "    return a.intersection(b).area / a.union(b).area\n",
    "\n",
    "#def iou(xy1,xy2):\n",
    "#    \n",
    "#    from shapely.geometry import Polygon\n",
    "#    \n",
    "#    boxA = Polygon(arrange_box(xy1[0],xy1[1])).buffer(1e-9)\n",
    "#    boxB = Polygon(arrange_box(xy2[0],xy2[1])).buffer(1e-9)\n",
    "#    \n",
    "#    try:\n",
    "#        return boxA.intersection(boxB).area / boxA.union(boxB).area\n",
    "#    except:\n",
    "#        print('Box 1:',xy1[0],xy1[1])\n",
    "#        print('Box 2:',xy2[0],xy2[1])\n",
    "#        sys.exit(1)\n",
    "\n",
    "def map_to_ground_truth(overlaps, print_it=False):\n",
    "    prior_overlap, prior_idx = overlaps.max(1)\n",
    "    if print_it: print(prior_overlap)\n",
    "#     pdb.set_trace()\n",
    "    gt_overlap, gt_idx = overlaps.max(0)\n",
    "    gt_overlap[prior_idx] = 1.99\n",
    "    for i,o in enumerate(prior_idx): gt_idx[o] = i\n",
    "    return gt_overlap,gt_idx\n",
    "\n",
    "def calculate_overlap(target_bb, predicted_bb):\n",
    "    overlaps = torch.zeros(target_bb.size(0),predicted_bb.size(0))\n",
    "\n",
    "    for j in range(overlaps.shape[0]):\n",
    "        for k in range(overlaps.shape[1]):\n",
    "            overlaps[j][k] = iou(target_bb[j],predicted_bb[k])\n",
    "            \n",
    "    return overlaps\n",
    "\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels.data.cpu()]\n",
    "\n",
    "from skimage import draw\n",
    "import numpy as np\n",
    "\n",
    "def poly2mask(vertex_row_coords, vertex_col_coords, shape):\n",
    "    fill_row_coords, fill_col_coords = draw.polygon(vertex_row_coords, vertex_col_coords, shape)\n",
    "    mask = torch.zeros(shape, dtype=np.bool)\n",
    "    mask[fill_row_coords, fill_col_coords] = True\n",
    "    return mask\n",
    "\n",
    "def convert_to_binary_mask(corners, shape=(800,800)):\n",
    "    point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2], corners[:, 0]])\n",
    "    x,y = point_squence.T[0].detach() * 10 + 400, -point_squence.T[1].detach() * 10 + 400\n",
    "    new_im = poly2mask(y, x, shape)\n",
    "    return new_im\n",
    "\n",
    "def create_conf_matrix(target, pred, debug=True):\n",
    "    import sys\n",
    "    \n",
    "    target = target.reshape(-1)\n",
    "    pred = pred.reshape(-1)\n",
    "    \n",
    "    if debug:\n",
    "        print('Target values:', target.unique())\n",
    "        print('Predicted values:', pred.unique())\n",
    "        print('Target shape:', target.shape)\n",
    "        print('Predicted shape:', pred.shape)\n",
    "    \n",
    "    nb_classes = max(target.unique())\n",
    "    if len(pred.unique()) > (nb_classes+1) :\n",
    "        print('More predicted classes than true classes')\n",
    "        sys.exit(1)\n",
    "        \n",
    "    conf_matrix = torch.zeros(nb_classes+1, nb_classes+1)\n",
    "    for t, p in zip(target, pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "def create_conf_matrix2(target, pred, debug=True):\n",
    "    import sys\n",
    "    \n",
    "    target = target.reshape(-1).cpu().numpy()\n",
    "    pred = pred.reshape(-1).cpu().numpy()\n",
    "    \n",
    "        \n",
    "    conf_matrix = torch.from_numpy(confusion_matrix(target, pred)).to(device)\n",
    "    \n",
    "    print('Threat Score: {}'.format((1.0*conf_matrix[1,1])/(conf_matrix[1,1]+conf_matrix[1,0]+conf_matrix[0,1])))\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "def classScores(conf_matrix):\n",
    "    print('Confusion matrix\\n', conf_matrix)\n",
    "    TP = conf_matrix.diag()\n",
    "    TN = torch.zeros_like(TP)\n",
    "    FP = torch.zeros_like(TP)\n",
    "    FN = torch.zeros_like(TP)\n",
    "    for c in range(conf_matrix.size(0)):\n",
    "        idx = torch.ones(conf_matrix.size(0)).byte()\n",
    "        idx[c] = 0\n",
    "        # all non-class samples classified as non-class\n",
    "        TN[c] = conf_matrix[idx.nonzero()[:, None], idx.nonzero()].sum() #conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n",
    "        # all non-class samples classified as class\n",
    "        FP[c] = conf_matrix[idx, c].sum()\n",
    "        # all class samples not classified as class\n",
    "        FN[c] = conf_matrix[c, idx].sum()\n",
    "\n",
    "        print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(\n",
    "            c, TP[c], TN[c], FP[c], FN[c]))\n",
    "        \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def split_list(a_list):\n",
    "    half = len(a_list)//2\n",
    "    return a_list[:half], a_list[half:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "You will get two different datasets:\n",
    "\n",
    " 1. an unlabeled dataset for pre-training\n",
    " 2. a labeled dataset for both training and validation\n",
    " \n",
    "## The dataset is organized into three levels: scene, sample and image\n",
    "\n",
    " 1. A scene is 25 seconds of a car's journey.\n",
    " 2. A sample is a snapshot of a scene at a given timeframe. Each scene will be divided into 126 samples, so about 0.2 seconds between consecutive samples.\n",
    " 3. Each sample contains 6 images captured by camera facing different orientation.\n",
    "    Each camera will capture 70 degree view. To make it simple, you can safely assume that the angle between the cameras is 60 degrees \n",
    "\n",
    "106 scenes in the unlabeled dataset and 28 scenes in the labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "\n",
    "train_scene_index = np.random.choice(labeled_scene_index, int(np.ceil(0.8*len(labeled_scene_index))))\n",
    "\n",
    "test_scene_index = labeled_scene_index[np.isin(labeled_scene_index, train_scene_index,invert=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabeled dataset\n",
    "\n",
    "You get two ways to access the dataset, by sample or by image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "transform=torchvision.transforms.Compose([torchvision.transforms.Resize((256,256)),\n",
    "                                          torchvision.transforms.ToTensor(),\n",
    "                              torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                             ])\n",
    "\n",
    "unlabeled_trainset = UnlabeledDataset(image_folder=image_folder, scene_index=unlabeled_scene_index, first_dim='sample', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=16, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# [batch_size, 6(images per sample), 3, H, W]\n",
    "sample = iter(trainloader).next()\n",
    "print(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(518, 776, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.utils.make_grid(sample[2], nrow=3).numpy().transpose(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get individual image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_trainset = UnlabeledDataset(image_folder=image_folder, scene_index=unlabeled_scene_index, first_dim='image', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=2, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# [batch_size, 3, H, W]\n",
    "image, camera_index = iter(trainloader).next()\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "# Camera_index is to tell you which camera is used. The order is\n",
    "# CAM_FRONT_LEFT, CAM_FRONT, CAM_FRONT_RIGHT, CAM_BACK_LEFT, CAM_BACK, CAM_BACK_RIGHT\n",
    "print(camera_index[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.imshow(image[0].numpy().transpose(1, 2, 0))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "labeled_trainset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2, \n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "labeled_testset = LabeledDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(labeled_testset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=2, \n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_image, extra = iter(trainloader).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kind of labels\n",
    "\n",
    " 1. The bounding box of surrounding objects\n",
    " 2. The binary road_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.imshow(torchvision.utils.make_grid(sample[0], nrow=3).numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 800, 800])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(road_image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 bias = True, \n",
    "                 pool=False,\n",
    "                 mp_kernel_size=2, \n",
    "                 mp_stride=2):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        if pool:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU(), \n",
    "                nn.MaxPool2d(kernel_size=mp_kernel_size, stride=mp_stride))\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU(), \n",
    "                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            torch.nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(negative_slope=0.1) ## nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ConvTLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 output_padding=0, \n",
    "                 unpool=False,\n",
    "                 mp_kernel_size=2, \n",
    "                 mp_stride=2):\n",
    "        super(ConvTLayer, self).__init__()\n",
    "        if unpool:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, \n",
    "                                   out_channels, \n",
    "                                   kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding, \n",
    "                                   output_padding=output_padding, \n",
    "                                   bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU()\n",
    "                nn.MaxUnpool2d(kernel_size=mp_kernel_size, stride=mp_stride)\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, \n",
    "                                   out_channels, \n",
    "                                   kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding, \n",
    "                                   output_padding=output_padding, \n",
    "                                   bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU()\n",
    "            )        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class Encoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.conv1 = ConvLayer(3,96, stride=2)\n",
    "        self.conv2 = ConvLayer(96,128, stride=2)\n",
    "        self.conv3 = ConvLayer(128,256, stride=2)\n",
    "        self.conv4 = ConvLayer(256,512, stride=2)\n",
    "        self.conv5 = ConvLayer(512,1024, padding=(0,0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "    \n",
    "class EncoderY(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.conv1 = ConvLayer(3,96, stride=2)\n",
    "        self.conv2 = ConvLayer(96,128, stride=2)\n",
    "        self.conv3 = ConvLayer(128,256, stride=2)\n",
    "        self.conv4 = ConvLayer(256,512, stride=2)\n",
    "        self.conv5 = ConvLayer(512,1024, padding=(0,0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.linear = nn.Linear(1024*7*7,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.linear(x.reshape(-1,1024*7*7))\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class Discriminator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator1, self).__init__()\n",
    "        self.encoder = Encoder1()\n",
    "        self.linear = nn.Linear(1024*13*13,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.linear(x.reshape(-1,1024*13*13))\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def random_vector(batch_size, length):\n",
    "    # Sample from a Gaussian distribution\n",
    "    z_vec = torch.randn(batch_size, length, 1, 1).float()\n",
    "    if torch.cuda.is_available():\n",
    "        z_vec = z_vec.to(device)\n",
    "    return z_vec\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.convt1 = ConvTLayer(4096, 2048, stride=2)\n",
    "        self.convt2 = ConvTLayer(2048, 1024, stride=2, output_padding=(0,0))\n",
    "        self.convt3 = ConvTLayer(1024, 512, stride=2, padding=(1,1), output_padding=(1,1))\n",
    "        self.convt4 = ConvTLayer(512, 256, stride=2, output_padding=(1,1))\n",
    "        self.convt5 = ConvTLayer(256, 128, stride=2, output_padding=(1,1))\n",
    "        self.convt6 = ConvTLayer(128, 96, stride=2, output_padding=(1,1))\n",
    "        self.convt7 = ConvTLayer(96, 64, stride=2, output_padding=(1,1))\n",
    "        self.convt8 = ConvTLayer(64, 32, stride=1, output_padding=(0,0))\n",
    "        self.convt9 = ConvTLayer(32, 18, stride=1, padding=(1,1), output_padding=(0,0))\n",
    "        \n",
    "    def forward(self,z):\n",
    "        z = self.convt1(z)\n",
    "        z = self.convt2(z)\n",
    "        z = self.convt3(z)\n",
    "        z = self.convt4(z)\n",
    "        z = self.convt5(z)\n",
    "        z = self.convt6(z)\n",
    "        z = self.convt7(z)\n",
    "        z = self.convt8(z)\n",
    "        z = self.convt9(z)\n",
    "        return z\n",
    "\n",
    "class Decoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder1, self).__init__()\n",
    "        self.convt1 = ConvTLayer(4096, 2048, stride=2)\n",
    "        self.convt2 = ConvTLayer(2048, 1024, stride=2, output_padding=(1,1))\n",
    "        self.convt3 = ConvTLayer(1024, 512, stride=2, padding=(1,1), output_padding=(0,0))\n",
    "        self.convt4 = ConvTLayer(512, 256, stride=2, output_padding=(0,0))\n",
    "        self.convt5 = ConvTLayer(256, 128, stride=2, output_padding=(0,0))\n",
    "        self.convt6 = ConvTLayer(128, 96, stride=2, output_padding=(0,0))\n",
    "        self.convt7 = ConvTLayer(96, 3, stride=2, output_padding=(1,1))\n",
    "        \n",
    "    def forward(self,z):\n",
    "        z = self.convt1(z)\n",
    "        z = self.convt2(z)\n",
    "        z = self.convt3(z)\n",
    "        z = self.convt4(z)\n",
    "        z = self.convt5(z)\n",
    "        z = self.convt6(z)\n",
    "        z = self.convt7(z)\n",
    "        return z\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.decoder(x)\n",
    "        return torch.tanh(x).reshape(6,-1,3,256,256)\n",
    "\n",
    "class Generator1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.decoder = Decoder1()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.decoder(x)\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderY(nn.Module):\n",
    "    def __init__(self,  d):\n",
    "        super(EncoderY, self).__init__()\n",
    "        self.conv1 = ConvLayer(3,96, stride=2)\n",
    "        self.conv2 = ConvLayer(96,128, stride=2)\n",
    "        self.conv3 = ConvLayer(128,256, stride=2)\n",
    "        self.conv4 = ConvLayer(256,512, stride=2)\n",
    "        self.conv5 = ConvLayer(512,1024, stride=2)\n",
    "        self.conv6 = ConvLayer(1024,2048, stride=2)\n",
    "        self.lin1 = nn.Linear(2048*3*3, d)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        #print(x.shape)\n",
    "        x = self.lin1(x.reshape(-1,2048*3*3))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderX(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(EncoderX, self).__init__()\n",
    "        self.conv1 = ConvLayer(1,16, stride=2)\n",
    "        self.conv2 = ConvLayer(16,32, stride=2)\n",
    "        self.conv3 = ConvLayer(32,48, stride=2)\n",
    "        self.conv4 = ConvLayer(48,64, stride=2)\n",
    "        self.conv5 = ConvLayer(64,96, stride=2)\n",
    "        self.conv6 = ConvLayer(96,128, stride=2)\n",
    "        self.conv7 = ConvLayer(128,256, stride=2)\n",
    "        self.conv8 = ConvLayer(256,512, stride=2)\n",
    "        self.lin1 = nn.Linear(512*2*2, d)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        #print(x.shape)\n",
    "        x = self.lin1(x.reshape(-1,512*2*2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderX(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderX, self).__init__()\n",
    "        self.convt1 = ConvTLayer(4096, 2048, kernel_size=3, stride=2)\n",
    "        self.convt2 = ConvTLayer(2048, 1024, kernel_size=3, stride=3, output_padding=(0,0))\n",
    "        self.convt3 = ConvTLayer(1024, 512, kernel_size=3, stride=2, padding=(1,1), output_padding=(0,0))\n",
    "        self.convt4 = ConvTLayer(512, 256, kernel_size=3, stride=3, padding=(1,1), output_padding=(0,0))\n",
    "        self.convt5 = ConvTLayer(256, 128, kernel_size=3, stride=2, output_padding=(0,0))\n",
    "        self.convt6 = ConvTLayer(128, 96, kernel_size=3, stride=2, output_padding=(0,0))\n",
    "        self.convt7 = ConvTLayer(96, 64, kernel_size=3, stride=2, output_padding=(0,0))\n",
    "        self.convt8 = ConvTLayer(64, 1, kernel_size=3, stride=2, output_padding=(1,1))\n",
    "        \n",
    "    def forward(self,z):\n",
    "        z = self.convt1(z)\n",
    "        z = self.convt2(z)\n",
    "        z = self.convt3(z)\n",
    "        z = self.convt4(z)\n",
    "        z = self.convt5(z)\n",
    "        z = self.convt6(z)\n",
    "        z = self.convt7(z)\n",
    "        z = self.convt8(z)\n",
    "        return torch.sigmoid(z)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "\n",
    "d = 20\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.y_encoder = nn.Sequential(\n",
    "            nn.Linear(3*256*256, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, d)\n",
    "        )\n",
    "\n",
    "        self.x_encoder = nn.Sequential(\n",
    "            nn.Linear(800*800, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, d * 2)\n",
    "        )\n",
    "\n",
    "        self.x_decoder = nn.Sequential(\n",
    "            nn.Linear(2*d, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, 800*800),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu_logvar = self.x_encoder(x.view(-1, 800*800)).view(-1, 2, d)\n",
    "        img_enc = self.y_encoder(y.view(-1, 3*256*256))\n",
    "        mu = mu_logvar[:, 0, :]\n",
    "        logvar = mu_logvar[:, 1, :]\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        out = torch.cat((z,img_enc),axis=1)\n",
    "        return self.x_decoder(out), mu, logvar\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "class CNN_VAE(nn.Module):\n",
    "    def __init__(self, hidden_d=286, image_d=625): #hidden_d=196, image_d=650 or hidden_d=286, image_d=625\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d = hidden_d\n",
    "        self.id = image_d\n",
    "        \n",
    "        self.y_encoder = EncoderY(d=self.id)\n",
    "\n",
    "        self.x_encoder = EncoderX(d=2*self.d)\n",
    "\n",
    "        self.x_decoder = DecoderX()\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu_logvar = self.x_encoder(x).view(-1, 2, self.d)\n",
    "        #print(mu_logvar.shape)\n",
    "        img_enc = [self.y_encoder(img.squeeze()) for img in y] \n",
    "        mu = mu_logvar[:, 0, :]\n",
    "        #print(mu.shape)\n",
    "        logvar = mu_logvar[:, 1, :]\n",
    "        #print(logvar.shape)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        img_enc.append(z)\n",
    "        out = torch.cat(img_enc,axis=1).reshape(-1,4096,1,1)\n",
    "        return self.x_decoder(out), mu, logvar\n",
    "    \n",
    "    def inference(self, y, mu=None, logvar=None):\n",
    "        N = y.size(1)\n",
    "        z = torch.randn((N, self.d)).to(device)\n",
    "        #print('Prior:',z.shape)\n",
    "        if mu is not None and logvar is not None:\n",
    "            #print(mu.shape)\n",
    "            #print(logvar.shape)\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            z = eps.mul(std).add_(mu)\n",
    "            #print('Post:',z.shape)\n",
    "        z = z.reshape(-1,196)\n",
    "        img_enc = [self.y_encoder(img.squeeze()) for img in y] \n",
    "        img_enc.append(z)\n",
    "        out = torch.cat(img_enc,axis=1).reshape(-1,4096,1,1)\n",
    "        return self.x_decoder(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 bias = True, \n",
    "                 pool=False,\n",
    "                 mp_kernel_size=2, \n",
    "                 mp_stride=2):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        if pool:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU(), \n",
    "                nn.MaxPool2d(kernel_size=mp_kernel_size, stride=mp_stride))\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU(), \n",
    "                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            torch.nn.Linear(in_features, out_features),\n",
    "            nn.BatchNorm1d(out_features),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.LeakyReLU(negative_slope=0.1) ## nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ConvTLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 out_channels, \n",
    "                 kernel_size=3, \n",
    "                 stride=1, \n",
    "                 padding=0, \n",
    "                 output_padding=0, \n",
    "                 unpool=False,\n",
    "                 mp_kernel_size=2, \n",
    "                 mp_stride=2):\n",
    "        super(ConvTLayer, self).__init__()\n",
    "        if unpool:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, \n",
    "                                   out_channels, \n",
    "                                   kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding, \n",
    "                                   output_padding=output_padding, \n",
    "                                   bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU()\n",
    "                nn.MaxUnpool2d(kernel_size=mp_kernel_size, stride=mp_stride)\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, \n",
    "                                   out_channels, \n",
    "                                   kernel_size, \n",
    "                                   stride=stride, \n",
    "                                   padding=padding, \n",
    "                                   output_padding=output_padding, \n",
    "                                   bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.LeakyReLU(negative_slope=0.1), ## nn.ReLU()\n",
    "            )        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class Encoder1(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super(Encoder1, self).__init__()\n",
    "        self.conv1 = ConvLayer(3,96, stride=2)\n",
    "        self.conv2 = ConvLayer(96,128, stride=2)\n",
    "        self.conv3 = ConvLayer(128,256, stride=2)\n",
    "        self.conv4 = ConvLayer(256,512, stride=2)\n",
    "        self.conv5 = ConvLayer(512,1024, stride=2)\n",
    "        self.conv6 = ConvLayer(1024,2048, stride=2)\n",
    "        self.lin1 = nn.Linear(2048*3*3, d)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.lin1(x.reshape(-1,2048*3*3))\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, d=650, output_size=4):\n",
    "        super(CNN, self).__init__()\n",
    "        self.encoder = Encoder1(d=d)\n",
    "        self.linear = nn.Linear(d,4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.linear(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d=650):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = Encoder1(d=d)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dict = torch.load('models/rotation_learning_model.pth', map_location=device)\n",
    "model_dict = encoder.state_dict()\n",
    "\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "encoder.load_state_dict(pretrained_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096 - 6*650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (encoder): Encoder1(\n",
       "    (conv1): ConvLayer(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (conv2): ConvLayer(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (conv3): ConvLayer(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (conv4): ConvLayer(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (conv5): ConvLayer(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (conv6): ConvLayer(\n",
       "      (layer): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Dropout(p=0.5, inplace=False)\n",
       "        (3): LeakyReLU(negative_slope=0.1)\n",
       "      )\n",
       "    )\n",
       "    (lin1): Linear(in_features=18432, out_features=650, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "\n",
    "class CNN_VAE_transfer(nn.Module):\n",
    "    def __init__(self, hidden_d=186, image_d=650): #hidden_d=196, image_d=650 or hidden_d=286, image_d=625\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d = hidden_d\n",
    "        self.id = image_d\n",
    "        \n",
    "        self.y_encoder = encoder\n",
    "\n",
    "        self.x_encoder = EncoderX(d=2*self.d)\n",
    "\n",
    "        self.x_decoder = DecoderX()\n",
    "\n",
    "    def reparameterise(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu_logvar = self.x_encoder(x).view(-1, 2, self.d)\n",
    "        #print(mu_logvar.shape)\n",
    "        img_enc = [self.y_encoder(img.squeeze()) for img in y] \n",
    "        mu = mu_logvar[:, 0, :]\n",
    "        #print(mu.shape)\n",
    "        logvar = mu_logvar[:, 1, :]\n",
    "        #print(logvar.shape)\n",
    "        z = self.reparameterise(mu, logvar)\n",
    "        img_enc.append(z)\n",
    "        out = torch.cat(img_enc,axis=1).reshape(-1,4096,1,1)\n",
    "        return self.x_decoder(out), mu, logvar\n",
    "    \n",
    "    def inference(self, y, mu=None, logvar=None):\n",
    "        N = y.size(1)\n",
    "        z = torch.randn((N, self.d)).to(device)\n",
    "        #print('Prior:',z.shape)\n",
    "        if mu is not None and logvar is not None:\n",
    "            #print(mu.shape)\n",
    "            #print(logvar.shape)\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            z = eps.mul(std).add_(mu)\n",
    "            #print('Post:',z.shape)\n",
    "        z = z.reshape(-1,196)\n",
    "        img_enc = [self.y_encoder(img.squeeze()) for img in y] \n",
    "        img_enc.append(z)\n",
    "        out = torch.cat(img_enc,axis=1).reshape(-1,4096,1,1)\n",
    "        return self.x_decoder(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "model = CNN_VAE_transfer().to(device)\n",
    "# Setting the optimiser\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "\n",
    "def loss_function(x_hat, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(\n",
    "        x_hat.view(-1, 800*800), x.view(-1, 800*800), reduction='sum'\n",
    "    )\n",
    "    KLD = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.stack(sample).reshape(6,-1,3,256,256)[0].to(device).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.stack(road_image).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred = model(1.0*torch.stack(road_image).reshape(-1,1,800,800).to(device),\n",
    "             torch.stack(sample).reshape(6,-1,3,256,256).to(device))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_hat, mu, logvar = pred\n",
    "loss = loss_function(x_hat, 1.0*torch.stack(road_image).to(device), mu, logvar)\n",
    "print(loss.item())\n",
    "# ===================backward====================\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training and testing the VAE\n",
    "\n",
    "accuracy_list = []\n",
    "threshold = 0.5\n",
    "epochs = 10\n",
    "codes = dict(μ=list(), logσ2=list(), y=list())\n",
    "for epoch in range(0, epochs + 1):\n",
    "    # Training\n",
    "    if epoch >= 0:  # test untrained net first\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            sample, target, road_image, extra = data\n",
    "            x = 1.0*torch.stack(road_image).reshape(-1, 1, 800, 800).to(device)\n",
    "            batch_size = x.size(0)\n",
    "            y = torch.stack(sample).reshape(6,-1,3,256,256).to(device)\n",
    "            # ===================forward=====================\n",
    "            x_hat, mu, logvar = model(x, y)\n",
    "            loss = loss_function(x_hat, x, mu, logvar)\n",
    "            train_loss += loss.item()\n",
    "            # ===================backward====================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # ===================log========================\n",
    "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(trainloader.dataset):.4f}')\n",
    "\n",
    "    means, logvars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss_post = 0\n",
    "        test_loss_prior = 0\n",
    "        road_correct_post = 0\n",
    "        road_correct_prior = 0\n",
    "        total_road = 0\n",
    "        conf_matrix_road = torch.zeros(2,2).to(device)\n",
    "        for batch_idx, data in enumerate(testloader):\n",
    "            sample, target, road_image, extra = data\n",
    "            x = 1.0*torch.stack(road_image).reshape(-1, 1, 800, 800).to(device)\n",
    "            batch_size = x.size(0)\n",
    "            y = torch.stack(sample).reshape(6,-1,3,256,256).to(device)\n",
    "            \n",
    "            # ===================forward=====================\n",
    "            mu = torch.mean(mu,0).repeat(batch_size).view(batch_size,196)\n",
    "            logvar = torch.mean(logvar,0).repeat(batch_size).view(batch_size,196)\n",
    "            x_hat_post = model.inference(y, mu, logvar)\n",
    "            x_hat_prior = model.inference(y)\n",
    "            test_loss_post += loss_function(x_hat_post, x, mu, logvar).item()\n",
    "            test_loss_prior += loss_function(x_hat_prior, x, mu, logvar).item()\n",
    "            # =====================log=======================\n",
    "            means.append(mu.detach())\n",
    "            logvars.append(logvar.detach())\n",
    "            \n",
    "            road_correct_post += (x_hat_post>threshold).eq((x==1).data.view_as((x_hat_post>threshold))).cpu().sum().item()\n",
    "            road_correct_prior += (x_hat_prior>threshold).eq((x==1).data.view_as((x_hat_prior>threshold))).cpu().sum().item()\n",
    "            total_road += x.nelement()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                for i in range(0,40):\n",
    "                    thld = 0.48+i*0.001\n",
    "                    print('Confusion Matrix (Post) at threshold: {}'.format(thld))\n",
    "                    print(create_conf_matrix2(1*(x==1), 1*(x_hat_post>thld)))\n",
    "                    print('='*50)\n",
    "                    print('Confusion Matrix (Prior) at threshold: {}'.format(thld))\n",
    "                    print(create_conf_matrix2(1*(x==1), 1*(x_hat_prior>thld)))\n",
    "                    print('='*50)\n",
    "                print('='*100)\n",
    "                print('='*100)\n",
    "            conf_matrix_road += create_conf_matrix2(1*(x==1), 1*(x_hat_post>threshold))\n",
    "                       \n",
    "    road_accuracy_post = 100. * road_correct_post / total_road\n",
    "    road_accuracy_prior = 100. * road_correct_prior / total_road\n",
    "\n",
    "    accuracy_list.append(road_accuracy_prior)\n",
    "    print(\"\"\"\\nTest set: Average loss: {:.4f}, \n",
    "    Accuracy Road (Post): {}/{} ({:.0f}%) ,\n",
    "    Accuracy Road (Prior): {}/{} ({:.0f}%) ,\n",
    "    Road: TP {} , \n",
    "    TN {}\n",
    "    FP {}\n",
    "    FN {}\"\"\".format(\n",
    "        test_loss_post, \n",
    "        road_correct_post, total_road, road_accuracy_post, \n",
    "        road_correct_prior, total_road, road_accuracy_prior, \n",
    "        *classScores(conf_matrix_road)))\n",
    "\n",
    "            #labels.append(y.detach())\n",
    "    # ===================log========================\n",
    "    codes['μ'].append(torch.cat(means))\n",
    "    codes['logσ2'].append(torch.cat(logvars))\n",
    "    #codes['y'].append(torch.cat(labels))\n",
    "    test_loss_post /= len(testloader.dataset)\n",
    "    test_loss_prior /= len(testloader.dataset)\n",
    "    print(f'====> Posterior Test set loss: {test_loss_post:.4f}')\n",
    "    print(f'====> Prior Test set loss: {test_loss_prior:.4f}')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow((x[0].squeeze()==1).detach().cpu().numpy(), cmap='binary')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow((x_hat_post[0].squeeze()>threshold).detach().cpu().numpy(), cmap='binary')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow((x_hat_prior[0].squeeze()>threshold).detach().cpu().numpy(), cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CDAutoEncoder(nn.Module):\n",
    "    r\"\"\"\n",
    "    Convolutional denoising autoencoder layer for stacked autoencoders.\n",
    "    This module is automatically trained when in model.training is True.\n",
    "\n",
    "    Args:\n",
    "        input_size: The number of features in the inputa\n",
    "        output_size: The number of features to output\n",
    "        stride: Stride of the convolutional layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, stride):\n",
    "        super(CDAutoEncoder, self).__init__()\n",
    "\n",
    "        self.forward_pass = nn.Sequential(\n",
    "            nn.Conv2d(input_size, output_size, kernel_size=2, stride=stride, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.backward_pass = nn.Sequential(\n",
    "            nn.ConvTranspose2d(output_size, input_size, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Train each autoencoder individually\n",
    "        x = x.detach()\n",
    "        # Add noise, but use the original lossless input as the target.\n",
    "        x_noisy = x * (Variable(x.data.new(x.size()).normal_(0, 0.1)) > -.1).type_as(x)\n",
    "        y = self.forward_pass(x_noisy)\n",
    "\n",
    "        if self.training:\n",
    "            x_reconstruct = self.backward_pass(y)\n",
    "            loss = self.criterion(x_reconstruct, Variable(x.data, requires_grad=False))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return y.detach()\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        return self.backward_pass(x)\n",
    "\n",
    "\n",
    "class StackedAutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A stacked autoencoder made from the convolutional denoising autoencoders above.\n",
    "    Each autoencoder is trained independently and at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StackedAutoEncoder, self).__init__()\n",
    "\n",
    "        self.ae1 = CDAutoEncoder(3, 128, 2)\n",
    "        self.ae2 = CDAutoEncoder(128, 256, 2)\n",
    "        self.ae3 = CDAutoEncoder(256, 512, 2)\n",
    "        self.ae3 = CDAutoEncoder(512, 1024, 2)\n",
    "        self.ae4 = CDAutoEncoder(1024, 2048, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.ae1(x)\n",
    "        a2 = self.ae2(a1)\n",
    "        a3 = self.ae3(a2)\n",
    "        a4 = self.ae4(a3)\n",
    "        a5 = self.ae4(a4)\n",
    "\n",
    "        if self.training:\n",
    "            return a5\n",
    "\n",
    "        else:\n",
    "            return a5, self.reconstruct(a5)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        a4_reconstruct = self.ae5.reconstruct(x)\n",
    "        a3_reconstruct = self.ae4.reconstruct(x)\n",
    "        a2_reconstruct = self.ae3.reconstruct(x)\n",
    "        a1_reconstruct = self.ae2.reconstruct(a2_reconstruct)\n",
    "        x_reconstruct = self.ae1.reconstruct(a1_reconstruct)\n",
    "        return x_reconstruct\n",
    "\n",
    "model = StackedAutoEncoder()\n",
    "#model = StackedAutoEncoder().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch % 10 == 0:\n",
    "        # Test the quality of our features with a randomly initialzed linear classifier.\n",
    "        classifier = nn.Sequential(nn.Linear(6*2056*4*4, 1000),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(1000, 800*800)).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    total_time = time.time()\n",
    "    correct = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        sample, target, road_image, extra = data\n",
    "        road_image = 1*torch.stack(road_image).to(device).reshape(-1,800*800)\n",
    "        batch_size = road_image.size(0)\n",
    "        sample = torch.stack(sample).reshape(6,-1,3,256,256).to(device)\n",
    "        features = []\n",
    "        for img in sample:\n",
    "            features.append(model(img).detach().view(features.size(0), -1))\n",
    "        features = torch.cat(features, axis=1)\n",
    "        prediction = classifier(features)\n",
    "        loss = criterion(prediction, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = prediction.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    total_time = time.time() - total_time\n",
    "\n",
    "    model.eval()\n",
    "    img, _ = data\n",
    "    img = Variable(img).cuda()\n",
    "    features, x_reconstructed = model(img)\n",
    "    reconstruction_loss = torch.mean((x_reconstructed.data - img.data)**2)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Saving epoch {}\".format(epoch))\n",
    "        orig = to_img(img.cpu().data)\n",
    "        save_image(orig, './imgs/orig_{}.png'.format(epoch))\n",
    "        pic = to_img(x_reconstructed.cpu().data)\n",
    "        save_image(pic, './imgs/reconstruction_{}.png'.format(epoch))\n",
    "\n",
    "    print(\"Epoch {} complete\\tTime: {:.4f}s\\t\\tLoss: {:.4f}\".format(epoch, total_time, reconstruction_loss))\n",
    "    print(\"Feature Statistics\\tMean: {:.4f}\\t\\tMax: {:.4f}\\t\\tSparsity: {:.4f}%\".format(\n",
    "        torch.mean(features.data), torch.max(features.data), torch.sum(features.data == 0.0)*100 / features.data.numel())\n",
    "    )\n",
    "    print(\"Linear classifier performance: {}/{} = {:.2f}%\".format(correct, len(dataloader)*batch_size, 100*float(correct) / (len(dataloader)*batch_size)))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "torch.save(model.state_dict(), './CDAE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,n_feature, hidden):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                           out_channels=n_feature, \n",
    "                           kernel_size=(3,5), \n",
    "                           stride=2)\n",
    "        self.maxp1 = nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "        self.conv1_bn = nn.BatchNorm2d(n_feature)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature*6, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=(3,5), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        self.maxp2 = nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=96, \n",
    "                               kernel_size=(3,3), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        self.maxp3 = nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "        self.conv3_bn = nn.BatchNorm2d(96)\n",
    "        \n",
    "        self.lin1 = nn.Linear(96*14*16,hidden)\n",
    "        self.lin1_bn = nn.BatchNorm1d(hidden)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = [y for y in x]\n",
    "        x = [F.relu(self.conv1_bn(self.maxp1(self.conv1(y)))) for y in x]\n",
    "        x = tuple(x)\n",
    "        x = torch.cat(x,axis=1)\n",
    "        x = F.relu(self.conv2_bn(self.maxp1(self.conv2(x))))\n",
    "        x = F.relu(self.conv3_bn(self.maxp1(self.conv3(x))))\n",
    "        x = F.relu(self.lin1_bn(self.lin1(x.reshape(-1,96*14*16))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden, hidden_img):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_img = hidden_img\n",
    "\n",
    "        self.lin1 = nn.Linear(hidden,6*3*hidden_img*int(1.2*(hidden_img)))\n",
    "        self.lin1_bn = nn.BatchNorm1d(6*3*hidden_img*int(1.2*(hidden_img)))\n",
    "\n",
    "        self.convT1 = nn.ConvTranspose2d(in_channels=3, \n",
    "                                 out_channels=3, \n",
    "                                 kernel_size=(3,3), \n",
    "                                 padding = (3,4),\n",
    "                                 stride=2, \n",
    "                                 dilation=(1,1),\n",
    "                                 output_padding=(0,0))\n",
    "        \n",
    "        self.convT1_bn = nn.BatchNorm2d(3)\n",
    "\n",
    "        self.convT2 = nn.ConvTranspose2d(in_channels=3, \n",
    "                                 out_channels=3, \n",
    "                                 kernel_size=(3,3), \n",
    "                                 padding = (3,3),\n",
    "                                 stride=2, \n",
    "                                 dilation=(1,1),\n",
    "                                 output_padding=(0,0))\n",
    "        \n",
    "        self.convT2_bn = nn.BatchNorm2d(3)\n",
    "\n",
    "        self.convT3 = nn.ConvTranspose2d(in_channels=3, \n",
    "                                 out_channels=3, \n",
    "                                 kernel_size=(3,3), \n",
    "                                 padding = (2,1),\n",
    "                                 stride=2, \n",
    "                                 dilation=(1,1),\n",
    "                                 output_padding=(1,1))\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = F.relu(self.lin1_bn(self.lin1(x)).reshape(6,-1,3,hidden_img,int(1.2*hidden_img)))\n",
    "        x = [y for y in x]\n",
    "        x = [self.convT3(\n",
    "            self.convT2_bn(self.convT2(\n",
    "            self.convT1_bn(self.convT1(y))))) for y in x]\n",
    "        x = tuple(x)\n",
    "        x = torch.stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, n_features=32, hidden=1000, hidden_img=36):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(n_features, hidden)\n",
    "        self.decoder = Decoder(hidden, hidden_img)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, criterion, optimizer, batch_size,show_photo=False):\n",
    "    model.train()\n",
    "    for batch_idx, (sample, target, road_image, extra) in enumerate(trainloader):\n",
    "        sample = torch.stack(sample).reshape(6,-1,3,256,306).to(device)\n",
    "        img = sample \n",
    "        img = img.to(device)\n",
    "        # img = img.view(img.size(0), -1)\n",
    "        # noise = do(torch.ones(img.shape))\n",
    "        # img_bad = (img * noise).to(device)  # comment out for standard AE\n",
    "        # ===================forward=====================\n",
    "        output = model(img)  # feed <img> (for std AE) or <img_bad> (for denoising AE)\n",
    "        loss = criterion(output, img.data)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print(f'epoch [{epoch + 1}/{num_epochs}], loss:{loss.item():.4f}')\n",
    "    plt_out = output.reshape(-1,6,3,256,306)\n",
    "    plt.imshow(torchvision.utils.make_grid(plt_out[0].detach().cpu()\n",
    "                                           , nrow=3).numpy().transpose(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Configure the optimiser\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train standard or denoising autoencoder (AE)\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convT1 = nn.ConvTranspose2d(in_channels=128, \n",
    "                                 out_channels=96, \n",
    "                                 kernel_size=1, \n",
    "                                 padding = (2,2),\n",
    "                                 stride=2, \n",
    "                                 dilation=(1,1),\n",
    "                                 output_padding=(0,0))\n",
    "\n",
    "## convT1(x).shape ## can add conv3(x) as a skip connection here\n",
    "\n",
    "convT2 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                 out_channels=96, \n",
    "                                 kernel_size=1, \n",
    "                                 padding = (4,4),\n",
    "                                 stride = 3, \n",
    "                                 dilation=(1,1),\n",
    "                                 output_padding=(0,0))\n",
    "\n",
    "convT3 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                 out_channels=96, \n",
    "                                 kernel_size=3, \n",
    "                                 padding = (2,2),\n",
    "                                 stride = 2, \n",
    "                                 dilation=(1,1),\n",
    "                                 output_padding=(1,1))\n",
    "\n",
    "convT4 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                 out_channels=64, \n",
    "                                 kernel_size=2, \n",
    "                                 padding = (0,0),\n",
    "                                 stride = 2, \n",
    "                                 dilation=(1,1),\n",
    "                                 output_padding=(0,0))\n",
    "\n",
    "convT5 = nn.ConvTranspose2d(in_channels=64, \n",
    "                                 out_channels=2, \n",
    "                                 kernel_size=2, \n",
    "                                 padding = (0,0),\n",
    "                                 stride = 2, \n",
    "                                 dilation=(1,1),\n",
    "                                 output_padding=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemSeg(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(SemSeg, self).__init__()\n",
    "        self.conv1 = conv1 = nn.Conv2d(in_channels=3, \n",
    "                                       out_channels=n_feature, \n",
    "                                       kernel_size=(3,7), \n",
    "                                       stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature*6, \n",
    "                               out_channels=2, \n",
    "                               kernel_size=(3,7), \n",
    "                               stride=2,\n",
    "                               padding=(2,3))\n",
    "        self.conv3 = nn.Conv2d(in_channels=2, \n",
    "                               out_channels=2, \n",
    "                               kernel_size=(1,5), \n",
    "                               stride=2,\n",
    "                               padding=(2,0))\n",
    "        self.conv4 = nn.Conv2d(in_channels=2, \n",
    "                               out_channels=2, \n",
    "                               kernel_size=(1,1), \n",
    "                               stride=2,\n",
    "                               padding=(2,2))\n",
    "        self.convT1 = nn.ConvTranspose2d(in_channels=2, \n",
    "                                         out_channels=2, \n",
    "                                         kernel_size=1, \n",
    "                                         stride=42, \n",
    "                                         dilation=1,\n",
    "                                         output_padding=1)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = [y for y in x]\n",
    "        x = [self.conv1(y) for y in x]\n",
    "        x = tuple(x)\n",
    "        x = torch.cat(x,axis=1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.convT1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemSegMulti(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(SemSegMulti, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                                       out_channels=n_feature, \n",
    "                                       kernel_size=(3,7), \n",
    "                                       stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature*6, \n",
    "                               out_channels=50, \n",
    "                               kernel_size=(3,7), \n",
    "                               stride=2,\n",
    "                               padding=(2,3))\n",
    "        self.conv3 = nn.Conv2d(in_channels=50, \n",
    "                               out_channels=25, \n",
    "                               kernel_size=(1,5), \n",
    "                               stride=2,\n",
    "                               padding=(2,0))\n",
    "        self.conv4 = nn.Conv2d(in_channels=25, \n",
    "                               out_channels=15, \n",
    "                               kernel_size=(1,1), \n",
    "                               stride=2,\n",
    "                               padding=(2,2))\n",
    "        self.convT1 = nn.ConvTranspose2d(in_channels=15, \n",
    "                                         out_channels=10, \n",
    "                                         kernel_size=1, \n",
    "                                         stride=42, \n",
    "                                         dilation=1,\n",
    "                                         output_padding=1)\n",
    "        self.convT2 = nn.ConvTranspose2d(in_channels=15, \n",
    "                                         out_channels=2, \n",
    "                                         kernel_size=1, \n",
    "                                         stride=42, \n",
    "                                         dilation=1,\n",
    "                                         output_padding=1)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = [y for y in x]\n",
    "        x = [self.conv1(y) for y in x]\n",
    "        x = tuple(x)\n",
    "        x = torch.cat(x,axis=1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        return [self.convT1(x),\n",
    "                self.convT2(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemSegMulti2(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(SemSegMulti2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                                       out_channels=n_feature, \n",
    "                                       kernel_size=(3,7), \n",
    "                                       stride=2)\n",
    "        self.conv1_bn = nn.BatchNorm2d(n_feature)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature*6, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=(3,7), \n",
    "                               stride=2,\n",
    "                               padding=(2,3))\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=96, \n",
    "                               kernel_size=(1,3), \n",
    "                               stride=2,\n",
    "                               padding=(4,0))\n",
    "        self.conv3_bn = nn.BatchNorm2d(96)\n",
    "        self.conv4 = nn.Conv2d(in_channels=96, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(1,1))\n",
    "        self.conv4_bn = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        self.conv5_bn = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        self.conv6_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.convT1 = nn.ConvTranspose2d(in_channels=128, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=1, \n",
    "                                         padding = (2,2),\n",
    "                                         stride=2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        self.convT1_bn = nn.BatchNorm2d(96)\n",
    "        ## convT1(x).shape ## can add conv3(x) as a skip connection here\n",
    "\n",
    "        self.convT2 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=1, \n",
    "                                         padding = (4,4),\n",
    "                                         stride = 3, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        self.convT2_bn = nn.BatchNorm2d(96)\n",
    "        self.convT3 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=3, \n",
    "                                         padding = (2,2),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(1,1))\n",
    "        self.convT3_bn = nn.BatchNorm2d(96)\n",
    "        self.convT4 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=64, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        self.convT4_bn = nn.BatchNorm2d(64)\n",
    "        self.convT5 = nn.ConvTranspose2d(in_channels=64, \n",
    "                                         out_channels=2, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        \n",
    "        self.convT6 = nn.ConvTranspose2d(in_channels=64, \n",
    "                                         out_channels=10, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = [y for y in x]\n",
    "        x = [F.relu(self.conv1_bn(self.conv1(y))) for y in x]\n",
    "        x = tuple(x)\n",
    "        x = torch.cat(x,axis=1)\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x1 = F.relu(self.conv4_bn(self.conv4(x)))\n",
    "        x1 = F.relu(self.conv5_bn(self.conv5(x1)))\n",
    "        x1 = F.relu(self.conv6_bn(self.conv6(x1)))\n",
    "        x1 = F.relu(self.convT1_bn(self.convT1(x1))) + x\n",
    "        return [\n",
    "            self.convT6(\n",
    "            F.relu(self.convT4_bn(self.convT4(\n",
    "            F.relu(self.convT3_bn(self.convT3(\n",
    "            F.relu(self.convT2_bn(self.convT2(x1)))))\n",
    "            ))))),\n",
    "            self.convT5(\n",
    "            F.relu(self.convT4_bn(self.convT4(\n",
    "            F.relu(self.convT3_bn(self.convT3(\n",
    "            F.relu(self.convT2_bn(self.convT2(x1)))))\n",
    "            )))))\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemSegMultiBB(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(SemSegMultiBB, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                                       out_channels=n_feature, \n",
    "                                       kernel_size=(3,7), \n",
    "                                       stride=2)\n",
    "        self.conv1_bn = nn.BatchNorm2d(n_feature)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature*6, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=(3,7), \n",
    "                               stride=2,\n",
    "                               padding=(2,3))\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=96, \n",
    "                               kernel_size=(1,3), \n",
    "                               stride=2,\n",
    "                               padding=(4,0))\n",
    "        self.conv3_bn = nn.BatchNorm2d(96)\n",
    "        self.conv4 = nn.Conv2d(in_channels=96, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(1,1))\n",
    "        self.conv4_bn = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        self.conv5_bn = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        self.conv6_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.convT1 = nn.ConvTranspose2d(in_channels=128, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=1, \n",
    "                                         padding = (2,2),\n",
    "                                         stride=2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        self.convT1_bn = nn.BatchNorm2d(96)\n",
    "        ## convT1(x).shape ## can add conv3(x) as a skip connection here\n",
    "\n",
    "        self.convT2 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=1, \n",
    "                                         padding = (4,4),\n",
    "                                         stride = 3, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        self.convT2_bn = nn.BatchNorm2d(96)\n",
    "        self.convT3 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=3, \n",
    "                                         padding = (2,2),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(1,1))\n",
    "        self.convT3_bn = nn.BatchNorm2d(96)\n",
    "        self.convT4 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=64, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        self.convT4_bn = nn.BatchNorm2d(64)\n",
    "        self.convT5 = nn.ConvTranspose2d(in_channels=64, \n",
    "                                         out_channels=2, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        \n",
    "        self.convT6 = nn.ConvTranspose2d(in_channels=64, \n",
    "                                         out_channels=2, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = [y for y in x]\n",
    "        x = [F.relu(self.conv1_bn(self.conv1(y))) for y in x]\n",
    "        x = tuple(x)\n",
    "        x = torch.cat(x,axis=1)\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x1 = F.relu(self.conv4_bn(self.conv4(x)))\n",
    "        x1 = F.relu(self.conv5_bn(self.conv5(x1)))\n",
    "        x1 = F.relu(self.conv6_bn(self.conv6(x1)))\n",
    "        x1 = F.relu(self.convT1_bn(self.convT1(x1))) + x\n",
    "        return [\n",
    "            self.convT6(\n",
    "            F.relu(self.convT4_bn(self.convT4(\n",
    "            F.relu(self.convT3_bn(self.convT3(\n",
    "            F.relu(self.convT2_bn(self.convT2(x1)))))\n",
    "            ))))),\n",
    "            self.convT5(\n",
    "            F.relu(self.convT4_bn(self.convT4(\n",
    "            F.relu(self.convT3_bn(self.convT3(\n",
    "            F.relu(self.convT2_bn(self.convT2(x1)))))\n",
    "            )))))\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemSegMultiBB2(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(SemSegMultiBB2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                                       out_channels=n_feature, \n",
    "                                       kernel_size=(3,7), \n",
    "                                       stride=2)\n",
    "        self.conv1_bn = nn.BatchNorm2d(n_feature)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature*6, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=(3,7), \n",
    "                               stride=2,\n",
    "                               padding=(2,3))\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=96, \n",
    "                               kernel_size=(1,3), \n",
    "                               stride=2,\n",
    "                               padding=(4,0))\n",
    "        self.conv3_bn = nn.BatchNorm2d(96)\n",
    "        self.conv4 = nn.Conv2d(in_channels=96, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(1,1))\n",
    "        self.conv4_bn = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        self.conv5_bn = nn.BatchNorm2d(128)\n",
    "        self.conv6 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        self.conv6_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.convT1 = nn.ConvTranspose2d(in_channels=128, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=1, \n",
    "                                         padding = (2,2),\n",
    "                                         stride=2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        self.convT1_bn = nn.BatchNorm2d(96)\n",
    "        ## convT1(x).shape ## can add conv3(x) as a skip connection here\n",
    "\n",
    "        self.convT2 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=1, \n",
    "                                         padding = (4,4),\n",
    "                                         stride = 3, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        self.convT2_bn = nn.BatchNorm2d(96)\n",
    "        self.convT3 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=3, \n",
    "                                         padding = (2,2),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(1,1))\n",
    "        self.convT3_bn = nn.BatchNorm2d(96)\n",
    "        self.convT4 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=64, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        self.convT4_bn = nn.BatchNorm2d(64)\n",
    "        self.convT5 = nn.ConvTranspose2d(in_channels=64, \n",
    "                                         out_channels=2, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        \n",
    "        self.convT6 = nn.ConvTranspose2d(in_channels=64, \n",
    "                                         out_channels=2, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = [y for y in x]\n",
    "        x = [F.relu(self.conv1_bn(self.conv1(y))) for y in x]\n",
    "        x = tuple(x)\n",
    "        x = torch.cat(x,axis=1)\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x1 = F.relu(self.conv4_bn(self.conv4(x)))\n",
    "        x1 = F.relu(self.conv5_bn(self.conv5(x1)))\n",
    "        x1 = F.relu(self.conv6_bn(self.conv6(x1)))\n",
    "        x1 = F.relu(self.convT1_bn(self.convT1(x1))) + x\n",
    "        return [\n",
    "            self.convT6(\n",
    "            F.relu(self.convT4_bn(self.convT4(\n",
    "            F.relu(self.convT3_bn(self.convT3(\n",
    "            F.relu(self.convT2_bn(self.convT2(x1)))))\n",
    "            ))))),\n",
    "            self.convT5(\n",
    "            F.relu(self.convT4_bn(self.convT4(\n",
    "            F.relu(self.convT3_bn(self.convT3(\n",
    "            F.relu(self.convT2_bn(self.convT2(x1)))))\n",
    "            )))))\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemSegVAE(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(SemSegVAE, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                                       out_channels=n_feature, \n",
    "                                       kernel_size=(3,7), \n",
    "                                       stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature*6, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=(3,7), \n",
    "                               stride=2,\n",
    "                               padding=(2,3))\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=96, \n",
    "                               kernel_size=(1,3), \n",
    "                               stride=2,\n",
    "                               padding=(4,0))\n",
    "        self.conv4 = nn.Conv2d(in_channels=96, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(1,1))\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        self.conv6 = nn.Conv2d(in_channels=128, \n",
    "                               out_channels=128, \n",
    "                               kernel_size=(7,7), \n",
    "                               stride=1,\n",
    "                               padding=(0,0))\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, d * 2)\n",
    "        )\n",
    "        \n",
    "        self.convT1 = nn.ConvTranspose2d(in_channels=128, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=1, \n",
    "                                         padding = (2,2),\n",
    "                                         stride=2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "\n",
    "        ## convT1(x).shape ## can add conv3(x) as a skip connection here\n",
    "\n",
    "        self.convT2 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=1, \n",
    "                                         padding = (4,4),\n",
    "                                         stride = 3, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "\n",
    "        self.convT3 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=96, \n",
    "                                         kernel_size=3, \n",
    "                                         padding = (2,2),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(1,1))\n",
    "\n",
    "        self.convT4 = nn.ConvTranspose2d(in_channels=96, \n",
    "                                         out_channels=64, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "\n",
    "        self.convT5 = nn.ConvTranspose2d(in_channels=64, \n",
    "                                         out_channels=2, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "        \n",
    "        self.convT6 = nn.ConvTranspose2d(in_channels=64, \n",
    "                                         out_channels=10, \n",
    "                                         kernel_size=2, \n",
    "                                         padding = (0,0),\n",
    "                                         stride = 2, \n",
    "                                         dilation=(1,1),\n",
    "                                         output_padding=(0,0))\n",
    "\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = [y for y in x]\n",
    "        x = [self.conv1(y) for y in x]\n",
    "        x = tuple(x)\n",
    "        x = torch.cat(x,axis=1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x1 = self.conv4(x)\n",
    "        x1 = self.conv5(x1)\n",
    "        x1 = self.conv6(x1)\n",
    "        x1 = self.convT1(x1) + x\n",
    "        return [self.convT6(self.convT4(self.convT3(self.convT2(x1)))),\n",
    "                self.convT5(self.convT4(self.convT3(self.convT2(x1))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, criterion, optimizer, batch_size):\n",
    "    model.train()\n",
    "    for batch_idx, (sample, target, road_image, extra) in enumerate(trainloader):\n",
    "        # send to device\n",
    "        sample = torch.stack(sample).reshape(6,-1,3,256,306).to(device)\n",
    "        road_image = 1*torch.stack(road_image).to(device)\n",
    "        output = model(sample)        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, road_image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * len(sample), len(trainloader.dataset),\n",
    "                #batch_idx * len(sample), len(trainloader),\n",
    "                100. * batch_idx * len(sample) / len(trainloader.dataset), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "def test(model, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (sample, target, road_image, extra) in enumerate(testloader):\n",
    "        # send to device\n",
    "        sample = torch.stack(sample).reshape(6,-1,3,256,306).to(device)\n",
    "        road_image = 1*torch.stack(road_image).to(device)\n",
    "        \n",
    "        output = model(sample) \n",
    "        test_loss += criterion(output, road_image).item() # sum up batch loss                                                               \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "        correct += pred.eq(road_image.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = 100. * correct / (16*800*800*len(testloader))\n",
    "    accuracy_list.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, (batch_size*800*800*len(testloader)),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model = SemSeg(n_feature=20)\n",
    "model.to(device)\n",
    "learning_rate = 1e-1\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model, criterion, optimizer, batch_size)\n",
    "    test(model, criterion, batch_size)\n",
    "    scheduler.step(accuracy_list[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMulti(epoch, model, criterion1, criterion2, optimizer, batch_size):\n",
    "    model.train()\n",
    "    for batch_idx, (sample, target, road_image, extra) in enumerate(trainloader):\n",
    "        # send to device\n",
    "        sample = torch.stack(sample).reshape(6,-1,3,256,306).to(device)\n",
    "        road_image = 1*torch.stack(road_image).to(device)\n",
    "        batch_size = sample.size(1)\n",
    "        y_target = torch.zeros((batch_size,10,800,800))\n",
    "        for i in range(batch_size):\n",
    "            for cat, bb in zip(target[i]['category'], target[i]['bounding_box']):\n",
    "                y_target[i,cat+1,:,:] += 1*convert_to_binary_mask(bb)\n",
    "        y_target = y_target.to(device)\n",
    "        values, indices = torch.max(y_target,1)\n",
    "        y_target = torch.max(y_target,1, keepdim=True)\n",
    "        y_targ = torch.zeros_like(values, dtype=torch.long)\n",
    "        y_targ[values > 0] = indices[values > 0]\n",
    "        y_target = y_targ\n",
    "        output = model(sample) \n",
    "        optimizer.zero_grad()\n",
    "        #print(y_target.unique())\n",
    "        loss1 = criterion1(output[0], y_target)\n",
    "        #print(road_image.unique())\n",
    "        loss2 = criterion2(output[1], road_image)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * batch_size, len(trainloader.dataset),\n",
    "                #batch_idx * len(sample), len(trainloader),\n",
    "                100. * batch_idx * batch_size / len(trainloader.dataset), \n",
    "                loss.item()))\n",
    "\n",
    "accuracy_list = []\n",
    "def testMulti(model, criterion1, criterion2):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    road_correct = 0\n",
    "    other_correct = 0\n",
    "    total_road = 0\n",
    "    total_other = 0\n",
    "    conf_matrix_road = torch.zeros(2, 2)\n",
    "    conf_matrix_other = torch.zeros(10, 10)\n",
    "    for batch_idx, (sample, target, road_image, extra) in enumerate(testloader):\n",
    "        # send to device\n",
    "        sample = torch.stack(sample).reshape(6,-1,3,256,306).to(device)\n",
    "        road_image = 1*torch.stack(road_image).to(device)\n",
    "        batch_size = sample.size(1)\n",
    "        y_target = torch.zeros((batch_size,10,800,800))\n",
    "        for i in range(batch_size):\n",
    "            for cat, bb in zip(target[i]['category'], target[i]['bounding_box']):\n",
    "                y_target[i,cat+1,:,:] += 1*convert_to_binary_mask(bb)\n",
    "        y_target = y_target.to(device)\n",
    "        values, indices = torch.max(y_target,1)\n",
    "        y_target = torch.max(y_target,1, keepdim=True)\n",
    "        y_targ = torch.zeros_like(values, dtype=torch.long)\n",
    "        y_targ[values > 0] = indices[values > 0]\n",
    "        y_target = y_targ\n",
    "        \n",
    "        output = model(sample)\n",
    "        \n",
    "        loss1 = criterion1(output[0], y_target)\n",
    "        #print(road_image.unique())\n",
    "        loss2 = criterion2(output[1], road_image)\n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        test_loss += loss.item() # sum up batch loss                                                               \n",
    "        pred_road = output[1].data.max(1, keepdim=True)[1] # get the index of the max log-probability  \n",
    "        pred_other = output[0].data.max(1, keepdim=True)[1] # get the index of the max log-probability  \n",
    "        road_correct += pred_road.eq(road_image.data.view_as(pred_road)).cpu().sum().item()\n",
    "        other_correct += pred_other.eq(y_target.data.view_as(pred_other)).cpu().sum().item()\n",
    "        total_road += road_image.nelement()\n",
    "        total_other += y_target.nelement()\n",
    "                \n",
    "        conf_matrix_road = create_conf_matrix2(road_image, pred_road)\n",
    "        conf_matrix_other = create_conf_matrix2(y_target, pred_other)\n",
    "                \n",
    "    test_loss /= len(testloader.dataset)\n",
    "    road_accuracy = 100. * road_correct / total_road\n",
    "    other_accuracy = 100. * other_correct / total_other\n",
    "    accuracy_list.append((road_accuracy + other_accuracy)/2)\n",
    "    print(\"\"\"\\nTest set: Average loss: {:.4f}, \n",
    "    Accuracy Road: {}/{} ({:.0f}%) , \n",
    "    Accuracy Other: {}/{}, ({:.0f}%),\n",
    "    Road: TP {} , \n",
    "    TN {}\n",
    "    FP {}\n",
    "    FN {},\n",
    "    Other: TP {} \n",
    "    TN {} \n",
    "    FP {}\n",
    "    FN {}\n",
    "    \\n\"\"\".format(\n",
    "        test_loss, road_correct, total_road, road_accuracy,\n",
    "        other_correct, total_other, other_accuracy, \n",
    "        *classScores(conf_matrix_road),\n",
    "        *classScores(conf_matrix_other)))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights1 = [0.1, 1.0]\n",
    "class_weights1 = torch.FloatTensor(weights1).cuda()\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss(reduction='mean',weight=class_weights1)\n",
    "\n",
    "weights = [0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "criterion2 = nn.CrossEntropyLoss(reduction='mean')\n",
    "model = SemSegMulti2(32)\n",
    "model.to(device)\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))\n",
    "\n",
    "for epoch in range(0, 50):\n",
    "    trainMulti(epoch, model, criterion1, criterion2, optimizer, batch_size)\n",
    "    testMulti(model, criterion1, criterion2)\n",
    "    scheduler.step(accuracy_list[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trainBB(epoch, model, criterion1, criterion2, optimizer, batch_size):\n",
    "    model.train()\n",
    "    for batch_idx, (sample, target, road_image, extra) in enumerate(trainloader):\n",
    "        # send to device\n",
    "        sample = torch.stack(sample).reshape(6,-1,3,256,306).to(device)\n",
    "        road_image = 1*torch.stack(road_image).to(device)\n",
    "        batch_size = sample.size(1)\n",
    "        y_target = torch.zeros((batch_size,1,800,800))\n",
    "        for i in range(batch_size):\n",
    "            for cat, bb in zip(target[i]['category'], target[i]['bounding_box']):\n",
    "                y_target[i,0,:,:] += 1*convert_to_binary_mask(bb)\n",
    "        y_target = 1*(y_target>0)\n",
    "        y_target = y_target.to(device).squeeze()\n",
    "        output = model(sample) \n",
    "        optimizer.zero_grad()\n",
    "        #print(y_target.unique())\n",
    "        loss1 = criterion1(output[0], y_target)\n",
    "        #print(road_image.unique())\n",
    "        loss2 = criterion2(output[1], road_image)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * batch_size, len(trainloader.dataset),\n",
    "                #batch_idx * len(sample), len(trainloader),\n",
    "                100. * batch_idx * batch_size / len(trainloader.dataset), \n",
    "                loss.item()))\n",
    "\n",
    "accuracy_list = []\n",
    "def testBB(model, criterion1, criterion2):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    road_correct = 0\n",
    "    other_correct = 0\n",
    "    total_road = 0\n",
    "    total_other = 0\n",
    "    conf_matrix_road = torch.zeros(2, 2)\n",
    "    conf_matrix_other = torch.zeros(10, 10)\n",
    "    for batch_idx, (sample, target, road_image, extra) in enumerate(testloader):\n",
    "        # send to device\n",
    "        sample = torch.stack(sample).reshape(6,-1,3,256,306).to(device)\n",
    "        road_image = 1*torch.stack(road_image).to(device)\n",
    "        batch_size = sample.size(1)\n",
    "        y_target = torch.zeros((batch_size,1,800,800))\n",
    "        for i in range(batch_size):\n",
    "            for cat, bb in zip(target[i]['category'], target[i]['bounding_box']):\n",
    "                y_target[i,0,:,:] += 1*convert_to_binary_mask(bb)\n",
    "        y_target = 1*(y_target>0)\n",
    "        y_target = y_target.to(device).squeeze()\n",
    "        \n",
    "        output = model(sample)\n",
    "        \n",
    "        loss1 = criterion1(output[0], y_target)\n",
    "        #print(road_image.unique())\n",
    "        loss2 = criterion2(output[1], road_image)\n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        test_loss += loss.item() # sum up batch loss                                                               \n",
    "        pred_road = output[1].data.max(1, keepdim=True)[1] # get the index of the max log-probability  \n",
    "        pred_other = output[0].data.max(1, keepdim=True)[1] # get the index of the max log-probability  \n",
    "        road_correct += pred_road.eq(road_image.data.view_as(pred_road)).cpu().sum().item()\n",
    "        other_correct += pred_other.eq(y_target.data.view_as(pred_other)).cpu().sum().item()\n",
    "        total_road += road_image.nelement()\n",
    "        total_other += y_target.nelement()\n",
    "                \n",
    "        conf_matrix_road = create_conf_matrix2(road_image, pred_road)\n",
    "        conf_matrix_other = create_conf_matrix2(y_target, pred_other)\n",
    "                \n",
    "    test_loss /= len(testloader.dataset)\n",
    "    road_accuracy = 100. * road_correct / total_road\n",
    "    other_accuracy = 100. * other_correct / total_other\n",
    "    accuracy_list.append((road_accuracy + other_accuracy)/2)\n",
    "    print(\"\"\"\\nTest set: Average loss: {:.4f}, \n",
    "    Accuracy Road: {}/{} ({:.0f}%) , \n",
    "    Accuracy Other: {}/{}, ({:.0f}%),\n",
    "    Road: TP {} , \n",
    "    TN {}\n",
    "    FP {}\n",
    "    FN {},\n",
    "    Other: TP {} \n",
    "    TN {} \n",
    "    FP {}\n",
    "    FN {}\n",
    "    \\n\"\"\".format(\n",
    "        test_loss, road_correct, total_road, road_accuracy,\n",
    "        other_correct, total_other, other_accuracy, \n",
    "        *classScores(conf_matrix_road),\n",
    "        *classScores(conf_matrix_other)))\n",
    "    \n",
    "\n",
    "weights1 = [0.6, 1.0]\n",
    "class_weights1 = torch.FloatTensor(weights1).cuda()\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss(reduction='mean',weight=class_weights1)\n",
    "\n",
    "weights2 = [0.02, 1.0]\n",
    "class_weights2 = torch.FloatTensor(weights2).cuda()\n",
    "criterion2 = nn.CrossEntropyLoss(reduction='mean',weight=class_weights2)\n",
    "model = SemSegMultiBB(32)\n",
    "model.to(device)\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))\n",
    "\n",
    "for epoch in range(0, 50):\n",
    "    trainBB(epoch, model, criterion1, criterion2, optimizer, batch_size)\n",
    "    testBB(model, criterion1, criterion2)\n",
    "    scheduler.step(accuracy_list[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, target, road_image, extra = iter(testloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = torch.zeros((batch_size,1,800,800))\n",
    "for i in range(batch_size):\n",
    "    for cat, bb in zip(target[i]['category'], target[i]['bounding_box']):\n",
    "        y_target[i,0,:,:] += 1*convert_to_binary_mask(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_target>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target = y_target>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for batch_idx, (sample, target, road_image, extra) in enumerate(testloader):\n",
    "    print(batch_idx, target[0]['category'])\n",
    "    batch_size = 1\n",
    "    y_target = torch.zeros((batch_size,9,800,800))\n",
    "    for i in range(batch_size):\n",
    "        for cat, bb in zip(target[i]['category'], target[i]['bounding_box']):\n",
    "            y_target[i,cat+1,:,:] += 1*convert_to_binary_mask(bb)\n",
    "            print(i, cat, y_target[i,cat+1,:,:].sum())\n",
    "    y_target = y_target.to(device)\n",
    "    values, indices = torch.max(y_target,1)\n",
    "    y_target = torch.max(y_target,1, keepdim=True)\n",
    "    y_targ = torch.zeros_like(values, dtype=torch.long)\n",
    "    y_targ[values > 0] = indices[values > 0]\n",
    "    print(y_targ)\n",
    "    print(y_targ.sum())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The shape of bounding box is [batch_size, N (the number of object), 2, 4]\n",
    "print(target[0]['bounding_box'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All bounding box are retangles\n",
    "# Each bounding box is organized with four corners of the box\n",
    "# All the values are in meter and bounded by 40 meters, and the origin is the center of ego car\n",
    "# the order of the four courners are front left, front right, back left and back right\n",
    "print(target[0]['bounding_box'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners = target[0]['bounding_box'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2], corners[:, 0]])\n",
    "x,y = point_squence.T[0].detach() * 10 + 400, -point_squence.T[1].detach() * 10 + 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    ax= plt.subplot(6,5 ,i+1)\n",
    "    new_im = convert_to_binary_mask(bb)\n",
    "    ax.imshow(new_im, cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each bounding box has a category\n",
    "# 'other_vehicle': 0,\n",
    "# 'bicycle': 1,\n",
    "# 'car': 2,\n",
    "# 'pedestrian': 3,\n",
    "# 'truck': 4,\n",
    "# 'bus': 5,\n",
    "# 'motorcycle': 6,\n",
    "# 'emergency_vehicle': 7,\n",
    "# 'animal': 8\n",
    "print(target[0]['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_seg_target = np.zeros((10,800,800))\n",
    "\n",
    "for cat, bb in zip(target[0]['category'], target[0]['bounding_box']):\n",
    "    sem_seg_target[cat,:,:] += convert_to_binary_mask(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(9):\n",
    "    ax=plt.subplot(3,3,i+1)\n",
    "    ax.imshow(sem_seg_target[i,:,:]==1, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Map Layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The road map layout is encoded into a binary array of size [800, 800] per sample \n",
    "# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car\n",
    "# The ego car is located in the center of the map (400, 400) and it is always facing the left\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(road_image[0], cmap='binary');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(road_image[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(road_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Info\n",
    "\n",
    "There is some extra information you can use in your model, but it is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action\n",
    "# Action is the label that what the object is doing\n",
    "\n",
    "# 'object_action_parked': 0,\n",
    "# 'object_action_driving_straight_forward': 1,\n",
    "# 'object_action_walking': 2,\n",
    "# 'object_action_running': 3,\n",
    "# 'object_action_lane_change_right': 4,\n",
    "# 'object_action_stopped': 5,\n",
    "# 'object_action_left_turn': 6,\n",
    "# 'object_action_right_turn': 7,\n",
    "# 'object_action_sitting': 8,\n",
    "# 'object_action_standing': 9,\n",
    "# 'object_action_gliding_on_wheels': 10,\n",
    "# 'object_action_abnormal_or_traffic_violation': 11,\n",
    "# 'object_action_lane_change_left': 12,\n",
    "# 'object_action_other_motion': 13,\n",
    "# 'object_action_reversing': 14,\n",
    "# 'object_action_u_turn': 15,\n",
    "# 'object_action_loss_of_control': 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extra[0]['action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ego Image\n",
    "# A more detailed ego image\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(extra[0]['ego_image'].numpy().transpose(1, 2, 0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lane Image\n",
    "# Binary lane image\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(extra[0]['lane_image'], cmap='binary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[0]['bounding_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The center of image is 400 * 400\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']\n",
    "\n",
    "ax.imshow(road_image[0], cmap ='binary');\n",
    "\n",
    "# The ego car position\n",
    "ax.plot(400, 400, 'x', color=\"red\")\n",
    "\n",
    "for i, bb in enumerate(target[0]['bounding_box']):\n",
    "    # You can check the implementation of the draw box to understand how it works \n",
    "    draw_box(ax, bb, color=color_list[target[0]['category'][i]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjDet(nn.Module):\n",
    "    def __init__(self,n_feature, n_categories, n_boxes):\n",
    "        super(ObjDet, self).__init__()\n",
    "        self.n_categories = n_categories\n",
    "        self.n_boxes = n_boxes\n",
    "        self.conv1 = conv1 = nn.Conv2d(in_channels=3, \n",
    "                                       out_channels=n_feature, \n",
    "                                       kernel_size=(3,7), \n",
    "                                       stride=2)\n",
    "        self.conv1_bn = nn.BatchNorm2d(n_feature)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature*6, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=(3,7), \n",
    "                               stride=2,\n",
    "                               padding=(2,3))\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=96, \n",
    "                               kernel_size=(1,5), \n",
    "                               stride=2,\n",
    "                               padding=(2,0))\n",
    "        self.conv3_bn = nn.BatchNorm2d(96)\n",
    "        self.conv4 = nn.Conv2d(in_channels=96, \n",
    "                               out_channels=96, \n",
    "                               kernel_size=(1,1), \n",
    "                               stride=2,\n",
    "                               padding=(2,2))\n",
    "        self.conv4_bn = nn.BatchNorm2d(96)\n",
    "        self.lin2 = nn.Linear(96*20*20, self.n_boxes*8)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = [y for y in x]\n",
    "        x = [F.relu(self.conv1_bn(self.conv1(y))) for y in x]\n",
    "        x = tuple(x)\n",
    "        x = torch.cat(x,axis=1)\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x = F.relu(self.conv4_bn(self.conv4(x)))\n",
    "        return self.lin2(x.reshape(-1,96*20*20)).reshape(-1,2,self.n_boxes*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat = 10\n",
    "n_boxes = 50\n",
    "n_feature = 20\n",
    "\n",
    "obj = ObjDet(n_feature, n_cat, n_boxes)\n",
    "\n",
    "cat_prob, xy = obj(torch.stack(sample)[0].reshape(6,-1,3,256,306))\n",
    "\n",
    "cat_prob = cat_prob.reshape(-1,n_cat)\n",
    "cat_prob.shape\n",
    "\n",
    "bb = xy.reshape(-1,2,4)\n",
    "bb.shape\n",
    "\n",
    "target_c = target[0]['category']\n",
    "print(target_c.shape)\n",
    "target_bb = target[0]['bounding_box'].reshape(-1,2,4)\n",
    "print(target_bb.shape)\n",
    "\n",
    "bb[0,:,:] = target_bb[3,:,:]\n",
    "bb[1,:,:] = target_bb[1,:,:]\n",
    "\n",
    "sample, target, road_image, extra = iter(trainloader).next()\n",
    "\n",
    "sample = torch.stack(sample)\n",
    "\n",
    "sample = sample.reshape(6,-1,3,256,306)\n",
    "\n",
    "batch_size = sample.size(1)\n",
    "\n",
    "pred = obj(sample)\n",
    "\n",
    "pred = tuple(zip(pred[0].reshape(batch_size,-1,10), pred[1].reshape(batch_size,-1,2,4)))\n",
    "\n",
    "targ = tuple([(targ_i['category'],targ_i['bounding_box']) for targ_i in target])\n",
    "\n",
    "loss = ssd_loss(pred,targ)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainObjDet(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (sample, target, road_image, extra) in enumerate(trainloader):\n",
    "        sample = torch.stack(sample).reshape(6,-1,3,256,306).to(device)\n",
    "        batch_size = sample.size(1)\n",
    "        pred = model(sample)\n",
    "        pred = tuple([bb for bb in pred.reshape(batch_size,-1,2,4)])\n",
    "        targ = tuple([targ_i['bounding_box'].to(device) for targ_i in target])\n",
    "        loss = ssd_loss(pred,targ)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * len(sample), len(trainloader.dataset),\n",
    "                #batch_idx * len(sample), len(trainloader),\n",
    "                100. * batch_idx * len(sample) / len(trainloader.dataset), \n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class BCE_Loss(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, pred, targ):\n",
    "        #print(targ)\n",
    "        t = one_hot_embedding(targ, self.num_classes).to(device)\n",
    "        x = pred.to(device)\n",
    "        w = self.get_weight(x,t)\n",
    "        if w is not None:\n",
    "            w = w.to(device)\n",
    "        #print(x.shape,t.shape)\n",
    "        return F.binary_cross_entropy_with_logits(x, t, w, size_average=False)/self.num_classes\n",
    "    \n",
    "    def get_weight(self,x,t): return None\n",
    "\n",
    "loss_f = BCE_Loss(n_cat)\n",
    "\n",
    "def ssd_1_loss(b_c,\n",
    "               b_bb,\n",
    "               bbox,\n",
    "               clas,\n",
    "               print_it=False):\n",
    "    overlaps = calculate_overlap(bbox.data, b_bb.data)\n",
    "    gt_overlap,gt_idx = map_to_ground_truth(overlaps,print_it)\n",
    "    gt_clas = clas[gt_idx]\n",
    "    pos = gt_overlap > 0.4\n",
    "    pos_idx = torch.nonzero(pos)[:,0]\n",
    "    #gt_clas[~pos] = n_cat\n",
    "    gt_bbox = bbox[gt_idx]\n",
    "    loc_loss = ((b_bb[pos_idx] - gt_bbox[pos_idx]).abs()).mean()\n",
    "    clas_loss  = loss_f(b_c, gt_clas)\n",
    "    return loc_loss, clas_loss\n",
    "\n",
    "def ssd_loss(pred,targ,print_it=False):\n",
    "    lcs,lls = 0.,0.\n",
    "    for b, t in zip(pred,targ):\n",
    "        b_c, b_bb = b\n",
    "        clas, bbox = t\n",
    "        #print(b_c.shape,b_bb.shape)\n",
    "        #print(clas.shape,bbox.shape)\n",
    "        loc_loss,clas_loss = ssd_1_loss(b_c,\n",
    "                                        b_bb,\n",
    "                                        bbox,\n",
    "                                        clas,\n",
    "                                        print_it)\n",
    "        lls += loc_loss\n",
    "        lcs += clas_loss\n",
    "    if print_it: print(f'loc: {lls.data[0]}, clas: {lcs.data[0]}')\n",
    "    return lls+lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat = 10\n",
    "n_boxes = 50\n",
    "n_feature = 20\n",
    "\n",
    "\n",
    "def ssd_1_loss(b_bb,\n",
    "               bbox,\n",
    "               print_it=False):\n",
    "    overlaps = calculate_overlap(bbox.data, b_bb.data)\n",
    "    gt_overlap,gt_idx = map_to_ground_truth(overlaps,print_it)\n",
    "    #gt_clas = clas[gt_idx]\n",
    "    pos = gt_overlap > 0.4\n",
    "    pos_idx = torch.nonzero(pos)[:,0]\n",
    "    #gt_clas[~pos] = n_cat\n",
    "    gt_bbox = bbox[gt_idx]\n",
    "    loc_loss = ((b_bb[pos_idx] - gt_bbox[pos_idx]).abs()).mean()\n",
    "    #clas_loss  = loss_f(b_c, gt_clas)\n",
    "    #return loc_loss, clas_loss\n",
    "    return loc_loss\n",
    "    \n",
    "def ssd_loss(pred,targ,print_it=False):\n",
    "    lls = 0.\n",
    "    for b, t in zip(pred,targ):\n",
    "        b_bb = b\n",
    "        bbox = t\n",
    "        #print(b_c.shape,b_bb.shape)\n",
    "        #print(clas.shape,bbox.shape)\n",
    "        loc_loss = ssd_1_loss(b_bb,\n",
    "                              bbox,\n",
    "                              print_it)\n",
    "        lls += loc_loss\n",
    "    if print_it: print(f'loc: {lls.data[0]}')\n",
    "    return lls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjDet(n_feature, n_cat, n_boxes)\n",
    "model.to(device)\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    ")\n",
    "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model)))\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    trainObjDet(epoch, model, optimizer)\n",
    "    #testObjDet(model, criterion, batch_size)\n",
    "    #scheduler.step(accuracy_list[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "During the whole competition, you have three submission deadlines. The dates will be announced on Piazza. You will have to fill up the template 'data_loader.py' for evaluation. (see the comment inside data_loader.py' for more information)\n",
    "\n",
    "There will be two leaderboards for the competition:\n",
    "The leaderboard for binary road map.\n",
    "We will evaluate your model's performance by using the average threat score (TS) across the test set:\n",
    "$$\\text{TS} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP} + \\text{FN}}$$\n",
    "The leaderboard for object detection:\n",
    "We will evaluate your model's performance for object detection by using the average mean threat score at different intersection over union (IoU) thresholds.\n",
    "There will be five different thresholds (0.5, 0.6, 0.7, 0.8, 0.9). For each thresholds, we will calculate the threat score. The final score will be a weighted average of all the threat scores:\n",
    "$$\\text{Final Score} = \\sum_t \\frac{1}{t} \\cdot \\frac{\\text{TP}(t)}{\\text{TP}(t) + \\text{FP}(t) + \\text{FN}(t)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "pdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
