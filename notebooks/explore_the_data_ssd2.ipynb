{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = [5, 5]\n",
    "matplotlib.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from data_helper import UnlabeledDataset, LabeledDataset\n",
    "from helper import collate_fn, draw_box\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0);\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "#image_folder = '../../DLSP20Dataset/data'\n",
    "#annotation_csv = '../../DLSP20Dataset/data/annotation.csv'\n",
    "\n",
    "\n",
    "# All the images are saved in image_folder\n",
    "# All the labels are saved in the annotation_csv file\n",
    "image_folder = '../../data'\n",
    "annotation_csv = '../../data/annotation.csv'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#device = 'cpu'\n",
    "\n",
    "#device = \"cpu\"\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "\n",
    "def order_points(pts):\n",
    "    from scipy.spatial import distance as dist\n",
    "    import numpy as np\n",
    "    \n",
    "    xSorted = pts[np.argsort(pts[:, 0]), :]\n",
    "\n",
    "    leftMost = xSorted[:2, :]\n",
    "    rightMost = xSorted[2:, :]\n",
    "\n",
    "    leftMost = leftMost[np.argsort(leftMost[:, 1]), :]\n",
    "    (tl, bl) = leftMost\n",
    "\n",
    "    D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0]\n",
    "    (br, tr) = rightMost[np.argsort(D)[::-1], :]\n",
    "\n",
    "    return np.array([tl, tr, br, bl]).astype(float)\n",
    "\n",
    "def arrange_box(x1,y1):\n",
    "    box=np.array(list(zip(x1,y1)))\n",
    "    box=order_points(box)\n",
    "    return box\n",
    "\n",
    "def iou(box1, box2):\n",
    "    from shapely.geometry import Polygon\n",
    "    try: \n",
    "        a = Polygon(torch.t(box1)).convex_hull\n",
    "        b = Polygon(torch.t(box2)).convex_hull\n",
    "\n",
    "        return a.intersection(b).area / a.union(b).area\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "#def iou(xy1,xy2):\n",
    "#    \n",
    "#    from shapely.geometry import Polygon\n",
    "#    \n",
    "#    boxA = Polygon(arrange_box(xy1[0],xy1[1])).buffer(1e-9)\n",
    "#    boxB = Polygon(arrange_box(xy2[0],xy2[1])).buffer(1e-9)\n",
    "#    \n",
    "#    try:\n",
    "#        return boxA.intersection(boxB).area / boxA.union(boxB).area\n",
    "#    except:\n",
    "#        print('Box 1:',xy1[0],xy1[1])\n",
    "#        print('Box 2:',xy2[0],xy2[1])\n",
    "#        sys.exit(1)\n",
    "\n",
    "def map_to_ground_truth(overlaps, print_it=False):\n",
    "    prior_overlap, prior_idx = overlaps.max(1)\n",
    "    if print_it: print(prior_overlap)\n",
    "#     pdb.set_trace()\n",
    "    gt_overlap, gt_idx = overlaps.max(0)\n",
    "    gt_overlap[prior_idx] = 1.99\n",
    "    for i,o in enumerate(prior_idx): gt_idx[o] = i\n",
    "    return gt_overlap,gt_idx\n",
    "\n",
    "def calculate_overlap(target_bb, predicted_bb):\n",
    "    overlaps = torch.zeros(target_bb.size(0),predicted_bb.size(0))\n",
    "\n",
    "    for j in range(overlaps.shape[0]):\n",
    "        for k in range(overlaps.shape[1]):\n",
    "            overlaps[j][k] = iou(target_bb[j],predicted_bb[k])\n",
    "            \n",
    "    return overlaps\n",
    "\n",
    "def one_hot_embedding(labels, num_classes):\n",
    "    return torch.eye(num_classes)[labels.data.cpu()]\n",
    "\n",
    "from skimage import draw\n",
    "import numpy as np\n",
    "\n",
    "def poly2mask(vertex_row_coords, vertex_col_coords, shape):\n",
    "    fill_row_coords, fill_col_coords = draw.polygon(vertex_row_coords, vertex_col_coords, shape)\n",
    "    mask = torch.zeros(shape, dtype=np.bool)\n",
    "    mask[fill_row_coords, fill_col_coords] = True\n",
    "    return mask\n",
    "\n",
    "def convert_to_binary_mask(corners, shape=(800,800)):\n",
    "    point_squence = torch.stack([corners[:, 0], corners[:, 1], corners[:, 3], corners[:, 2], corners[:, 0]])\n",
    "    x,y = point_squence.T[0].detach() * 10 + 400, -point_squence.T[1].detach() * 10 + 400\n",
    "    new_im = poly2mask(y, x, shape)\n",
    "    return new_im\n",
    "\n",
    "def create_conf_matrix(target, pred, debug=True):\n",
    "    import sys\n",
    "    \n",
    "    target = target.reshape(-1)\n",
    "    pred = pred.reshape(-1)\n",
    "    \n",
    "    if debug:\n",
    "        print('Target values:', target.unique())\n",
    "        print('Predicted values:', pred.unique())\n",
    "        print('Target shape:', target.shape)\n",
    "        print('Predicted shape:', pred.shape)\n",
    "    \n",
    "    nb_classes = max(target.unique())\n",
    "    if len(pred.unique()) > (nb_classes+1) :\n",
    "        print('More predicted classes than true classes')\n",
    "        sys.exit(1)\n",
    "        \n",
    "    conf_matrix = torch.zeros(nb_classes+1, nb_classes+1)\n",
    "    for t, p in zip(target, pred):\n",
    "        conf_matrix[t, p] += 1\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "def create_conf_matrix2(target, pred, debug=True):\n",
    "    import sys\n",
    "    \n",
    "    target = target.reshape(-1).cpu().numpy()\n",
    "    pred = pred.reshape(-1).cpu().numpy()\n",
    "    \n",
    "        \n",
    "    conf_matrix = torch.from_numpy(confusion_matrix(target, pred)).to(device)\n",
    "    \n",
    "    return conf_matrix\n",
    "\n",
    "def classScores(conf_matrix):\n",
    "    print('Confusion matrix\\n', conf_matrix)\n",
    "    TP = conf_matrix.diag()\n",
    "    TN = torch.zeros_like(TP)\n",
    "    FP = torch.zeros_like(TP)\n",
    "    FN = torch.zeros_like(TP)\n",
    "    for c in range(conf_matrix.size(0)):\n",
    "        idx = torch.ones(conf_matrix.size(0)).byte()\n",
    "        idx[c] = 0\n",
    "        # all non-class samples classified as non-class\n",
    "        TN[c] = conf_matrix[idx.nonzero()[:, None], idx.nonzero()].sum() #conf_matrix[idx[:, None], idx].sum() - conf_matrix[idx, c].sum()\n",
    "        # all non-class samples classified as class\n",
    "        FP[c] = conf_matrix[idx, c].sum()\n",
    "        # all class samples not classified as class\n",
    "        FN[c] = conf_matrix[c, idx].sum()\n",
    "\n",
    "        print('Class {}\\nTP {}, TN {}, FP {}, FN {}'.format(\n",
    "            c, TP[c], TN[c], FP[c], FN[c]))\n",
    "        \n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "def IOU(bbox1, bbox2):\n",
    "    '''Calculate overlap between two bounding boxes [x, y, w, h] as the area of intersection over the area of unity'''\n",
    "    x1, y1, w1, h1 = bbox1[0], bbox1[1], bbox1[2], bbox1[3]  # TODO: Check if its more performant if tensor elements are accessed directly below.\n",
    "    x2, y2, w2, h2 = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n",
    "\n",
    "    w_I = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
    "    h_I = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
    "    w_I = max(w_I, 0)  # set w_I and h_I zero if there is no intersection\n",
    "    h_I = max(h_I, 0)\n",
    "    I = w_I * h_I\n",
    "\n",
    "    U = w1 * h1 + w2 * h2 - I\n",
    "\n",
    "    return I / U\n",
    "\n",
    "def dist(bbox1, bbox2):\n",
    "    return torch.sqrt(torch.sum(torch.square(bbox1[:2] - bbox2[:2])))\n",
    "\n",
    "# Dataset\n",
    "\n",
    "# You shouldn't change the unlabeled_scene_index\n",
    "# The first 106 scenes are unlabeled\n",
    "unlabeled_scene_index = np.arange(106)\n",
    "# The scenes from 106 - 133 are labeled\n",
    "# You should devide the labeled_scene_index into two subsets (training and validation)\n",
    "labeled_scene_index = np.arange(106, 134)\n",
    "\n",
    "train_scene_index = np.random.choice(labeled_scene_index, int(np.ceil(0.9*len(labeled_scene_index))))\n",
    "\n",
    "test_scene_index = labeled_scene_index[np.isin(labeled_scene_index, train_scene_index,invert=True)]\n",
    "#test_scene_index = train_scene_index\n",
    "\n",
    "\n",
    "transform=torchvision.transforms.Compose([torchvision.transforms.Resize((100,100)),\n",
    "                                          torchvision.transforms.ToTensor(),\n",
    "                              #torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                             ])\n",
    "\n",
    "\n",
    "# Labeled dataset\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "NUM_SAMPLE_PER_SCENE = 126\n",
    "NUM_IMAGE_PER_SAMPLE = 6\n",
    "image_names = [\n",
    "    'CAM_FRONT_LEFT.jpeg',\n",
    "    'CAM_FRONT.jpeg',\n",
    "    'CAM_FRONT_RIGHT.jpeg',\n",
    "    'CAM_BACK_LEFT.jpeg',\n",
    "    'CAM_BACK.jpeg',\n",
    "    'CAM_BACK_RIGHT.jpeg',\n",
    "    ]\n",
    "\n",
    "def mapX(x):\n",
    "    return (x+40)/80\n",
    "\n",
    "def mapY(x):\n",
    "    return (x+40)/80\n",
    "\n",
    "def boxToHWC(boxes1):\n",
    "    boxes1_max_x = mapX(boxes1[:, 0].max(dim=1)[0])\n",
    "    boxes1_min_x = mapX(boxes1[:, 0].min(dim=1)[0])\n",
    "    boxes1_max_y = mapY(boxes1[:, 1].max(dim=1)[0])\n",
    "    boxes1_min_y = mapY(boxes1[:, 1].min(dim=1)[0])\n",
    "    \n",
    "    x = (boxes1_max_x + boxes1_min_x)/2\n",
    "    y = (boxes1_max_y + boxes1_min_y)/2\n",
    "    h = (boxes1_max_y - boxes1_min_y)\n",
    "    w = (boxes1_max_x - boxes1_min_x)\n",
    "    \n",
    "    return torch.stack((x,y,h,w)).T\n",
    "\n",
    "\n",
    "def imgToHWC(img):\n",
    "    height, width = img.shape[:2]     \n",
    "    x1 = int(row['XMin'] * width)     \n",
    "    x2 = int(row['XMax'] * width)     \n",
    "    y1 = int(row['YMin'] * height)     \n",
    "    y2 = int(row['YMax'] * height)\n",
    "    return (x1+x2)/2, (y1+y2)/2, height, width\n",
    "\n",
    "\n",
    "def transformTargetstoHWC(targets):\n",
    "    bounding_box = targets[\"bounding_box\"]\n",
    "    max, ind = torch.max(bounding_box, dim=3)\n",
    "    min, ind = torch.min(bounding_box, dim=3)\n",
    "    c = (400 + ((max + min)/2) * 10) / 1.92\n",
    "    w = (max - min) * 10 / 1.92\n",
    "    target = torch.cat((c, w), dim=2)\n",
    "    category = targets[\"category\"]\n",
    "    targets = torch.cat((target, category.unsqueeze(2).double()), dim=2).float()\n",
    "    target_lengths = torch.tensor(targets.numpy().shape[:2])\n",
    "    return targets, target_lengths\n",
    "\n",
    "\n",
    "# The dataset class for labeled data.\n",
    "class LabeledSSDDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, image_folder, annotation_file, scene_index, transform, extra_info=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_folder (string): the location of the image folder\n",
    "            annotation_file (string): the location of the annotations\n",
    "            scene_index (list): a list of scene indices for the unlabeled data \n",
    "            transform (Transform): The function to process the image\n",
    "            extra_info (Boolean): whether you want the extra information\n",
    "        \"\"\"\n",
    "        \n",
    "        self.image_folder = image_folder\n",
    "        self.annotation_dataframe = pd.read_csv(annotation_file)\n",
    "        self.scene_index = scene_index\n",
    "        self.transform = transform\n",
    "        self.extra_info = extra_info\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.scene_index.size * NUM_SAMPLE_PER_SCENE\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        scene_id = self.scene_index[index // NUM_SAMPLE_PER_SCENE]\n",
    "        sample_id = index % NUM_SAMPLE_PER_SCENE\n",
    "        sample_path = os.path.join(self.image_folder, f'scene_{scene_id}', f'sample_{sample_id}') \n",
    "\n",
    "        images = []\n",
    "        for image_name in image_names:\n",
    "            image_path = os.path.join(sample_path, image_name)\n",
    "            image = Image.open(image_path)\n",
    "            images.append(self.transform(image))\n",
    "        image_tensor = torch.stack(images)\n",
    "\n",
    "        data_entries = self.annotation_dataframe[(self.annotation_dataframe['scene'] == scene_id) & (self.annotation_dataframe['sample'] == sample_id)]\n",
    "        corners = data_entries[['fl_x', 'fr_x', 'bl_x', 'br_x', 'fl_y', 'fr_y','bl_y', 'br_y']].to_numpy()\n",
    "        categories = data_entries.category_id.to_numpy()\n",
    "        \n",
    "        ego_path = os.path.join(sample_path, 'ego.png')\n",
    "        ego_image = Image.open(ego_path)\n",
    "        ego_image = torchvision.transforms.functional.to_tensor(ego_image)\n",
    "        #road_image = convert_map_to_road_map(ego_image)\n",
    "        \n",
    "        target = {}\n",
    "        target['bounding_box'] = torch.as_tensor(corners).view(-1, 2, 4)\n",
    "        target['category'] = torch.as_tensor(categories).view(1,-1)\n",
    "        \n",
    "        target['chw'] = boxToHWC(target['bounding_box']).unsqueeze(0).double()\n",
    "\n",
    "\n",
    "        return image_tensor, target['chw'], target['bounding_box'], target['category']\n",
    "        \n",
    "\n",
    "# The labeled dataset can only be retrieved by sample.\n",
    "# And all the returned data are tuple of tensors, since bounding boxes may have different size\n",
    "# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.\n",
    "labeled_trainset = LabeledSSDDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=train_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(labeled_trainset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0, \n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "labeled_testset = LabeledSSDDataset(image_folder=image_folder,\n",
    "                                  annotation_file=annotation_csv,\n",
    "                                  scene_index=test_scene_index,\n",
    "                                  transform=transform,\n",
    "                                  extra_info=True\n",
    "                                 )\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(labeled_testset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=0, \n",
    "                                          collate_fn=collate_fn)\n",
    "\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.LeakyReLU(negative_slope=0.1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        #nn.Dropout(0.5),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.LeakyReLU(negative_slope=0.1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        #nn.Dropout(0.5)\n",
    "    ) \n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, d=650, output_size=4):\n",
    "        super().__init__()\n",
    "        self.dconv_down1 = double_conv(3, 16)\n",
    "        self.dconv_down2 = double_conv(16, 32)\n",
    "        self.dconv_down3 = double_conv(32, 48)\n",
    "        self.dconv_down4 = double_conv(48, 64)        \n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.linear1 = nn.Linear(6*64*12*12,4*88) ## 88 boxes\n",
    "        self.linear2 = nn.Linear(6*64*12*12,88) ## 88 boxes\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(conv1)\n",
    "\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(conv2)\n",
    "\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(conv3)  \n",
    "\n",
    "        \n",
    "        x = self.dconv_down4(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = [self.forward_once(y) for y in x]\n",
    "        x = torch.cat(x,axis=1)\n",
    "        x1 = self.linear1(x.reshape(-1,6*64*12*12))\n",
    "        x2 = self.linear2(x.reshape(-1,6*64*12*12))\n",
    "        return (torch.sigmoid(x1),torch.sigmoid(x2))\n",
    "\n",
    "model = CNN()\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                       mode='min', \n",
    "                                                       factor=0.1, \n",
    "                                                       patience=3,\n",
    "                                                       verbose=True)\n",
    "\n",
    "best_loss = 1000000000000000\n",
    "num_epochs = 50\n",
    "threshold = 0.5\n",
    "num_objects = 88\n",
    "best_iou = 0\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = {}\n",
    "    train_loss['boxes'] = 0\n",
    "    train_loss['probs'] = 0\n",
    "    train_loss['total'] = 0\n",
    "    for (x, bb_true, bb_orig, class_true) in trainloader:\n",
    "        exp = torch.stack(bb_true)\n",
    "        exp = exp.reshape(1, -1, 4)\n",
    "        pred, probs = model(torch.stack(x).reshape(6,-1,3,100,100))\n",
    "        pred = pred.reshape(1, -1, 4)\n",
    "        probs = probs.reshape(1, -1, 88)\n",
    "        num_true = exp.size(1)\n",
    "        pred = pred.reshape(num_objects, -1)\n",
    "        exp = exp.reshape(num_true, -1)      \n",
    "        pred_bboxes = pred[:, :4]\n",
    "        exp_bboxes = exp[:, :4]\n",
    "        # TODO: Try flipping array and see if results differ.\n",
    "        ious = np.zeros((num_true, num_objects))\n",
    "        for i, exp_bbox in enumerate(exp_bboxes):\n",
    "            for j, pred_bbox in enumerate(pred_bboxes):\n",
    "                ious[i, j] = IOU(exp_bbox, pred_bbox)\n",
    "\n",
    "        exp_idx, pred_idx = linear_sum_assignment(-ious)\n",
    "    \n",
    "\n",
    "        loss1 = mse(exp[exp_idx],pred[pred_idx])\n",
    "        train_loss['boxes'] += loss1.item()\n",
    "            \n",
    "        label = torch.zeros(88)\n",
    "        label[pred_idx] = 1\n",
    "        loss2 = mse(probs.reshape(-1), label.reshape(-1))\n",
    "        train_loss['probs'] = loss2.item()\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        train_loss['total'] = loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('====> Epoch: {} Average loss (boxes): {} / {}'.format(\n",
    "    epoch,train_loss['boxes'],len(trainloader.dataset)))\n",
    "    print('====> Epoch: {} Average loss (probs): {} / {}'.format(\n",
    "    epoch,train_loss['probs'],len(trainloader.dataset)))\n",
    "    print('====> Epoch: {} Average loss (total): {} / {}'.format(\n",
    "    epoch,train_loss['total'],len(trainloader.dataset)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = {}\n",
    "        test_loss['boxes'] = 0\n",
    "        test_loss['probs'] = 0\n",
    "        test_loss['total'] = 0\n",
    "        iou_list = []\n",
    "        for (x, bb_true, bb_orig, class_true) in testloader:\n",
    "            exp = torch.stack(bb_true)\n",
    "            exp = exp.reshape(1, -1, 4)\n",
    "            pred, probs = model(torch.stack(x).reshape(6,-1,3,100,100))\n",
    "            pred = pred.reshape(1, -1, 4)\n",
    "            probs = probs.reshape(1, -1, 88)\n",
    "            num_true = exp.size(1)\n",
    "            pred = pred.reshape(num_objects, -1)\n",
    "            exp = exp.reshape(num_true, -1)      \n",
    "            pred_bboxes = pred[:, :4]\n",
    "            exp_bboxes = exp[:, :4]\n",
    "\n",
    "            pred_idx = (probs>threshold).squeeze()\n",
    "            predicted_boxes = pred[pred_idx]\n",
    "            predicted_num_objects = torch.sum(probs>threshold)\n",
    "            ious = np.zeros((num_true, predicted_num_objects))\n",
    "            for i, exp_bbox in enumerate(exp_bboxes):\n",
    "                for j, pred_bbox in enumerate(predicted_boxes):\n",
    "                    ious[i, j] = IOU(exp_bbox, pred_bbox)\n",
    "                    \n",
    "            iou_list.append(np.mean(ious))\n",
    "        mean_iou = sum(iou_list) / len(iou_list)\n",
    "        print('====> Epoch: {} Average IOU on Test Set: {}'.format(epoch, mean_iou))\n",
    "        if mean_iou > best_iou:\n",
    "            best_iou = mean_iou\n",
    "            torch.save(model.state_dict(), 'models/best_simple_object_detector.pth')\n",
    "        scheduler.step(mean_iou)\n",
    "            \n",
    "torch.save(model.state_dict(), 'models/final_simple_object_detector.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "pdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
